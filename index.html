<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
<script src="https://telegram.org/js/telegram-web-app.js"></script>

<style>
body {
    background: #fff;
    color: #111;
    font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    height: 100vh;
    margin: 0;
    overflow: hidden;
}

#orbWrap {
    width: 220px;
    height: 220px;
    margin-bottom: 20px;
}

canvas {
    width: 100%;
    height: 100%;
}

#status {
    font-size: 13px;
    opacity: 0.6;
    margin-bottom: 8px;
}

#transcript {
    font-size: 12px;
    max-width: 80%;
    text-align: center;
    opacity: 0.45;
    white-space: pre-wrap;
}
</style>
</head>

<body>

<div id="orbWrap">
    <canvas id="orb"></canvas>
</div>
<div id="status">initializing…</div>
<div id="transcript"></div>

<script>
const tg = window.Telegram.WebApp;
tg.expand();

const statusEl = document.getElementById("status");
const transcriptEl = document.getElementById("transcript");

const canvas = document.getElementById("orb");
const ctx = canvas.getContext("2d");
canvas.width = 400;
canvas.height = 400;

let state = "idle";
let energy = 0;

// === AUDIO REACTIVE CORE ===
const AudioCtx = window.AudioContext || window.webkitAudioContext;
const audioCtx = new AudioCtx();
const analyser = audioCtx.createAnalyser();
analyser.fftSize = 256;
const spectrum = new Uint8Array(analyser.frequencyBinCount);

function draw(t) {
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    analyser.getByteFrequencyData(spectrum);
    const avg = spectrum.reduce((a,b)=>a+b,0) / spectrum.length / 255;
    energy += (avg - energy) * 0.08;

    const cx = canvas.width / 2;
    const cy = canvas.height / 2;
    const r = 70 + energy * 80;

    const g = ctx.createRadialGradient(cx, cy, r * 0.15, cx, cy, r);
    g.addColorStop(0, `rgba(0,0,0,${0.4 + energy})`);
    g.addColorStop(0.6, `rgba(0,0,0,${0.2 + energy * 0.6})`);
    g.addColorStop(1, "rgba(0,0,0,0)");

    ctx.fillStyle = g;
    ctx.beginPath();
    ctx.arc(cx, cy, r, 0, Math.PI * 2);
    ctx.fill();

    if (state === "listening") {
        ctx.strokeStyle = "rgba(0,0,0,0.25)";
        ctx.lineWidth = 2 + energy * 6;
        ctx.beginPath();
        ctx.arc(cx, cy, r + 12 + Math.sin(t / 180) * 8, 0, Math.PI * 2);
        ctx.stroke();
    }

    requestAnimationFrame(draw);
}
requestAnimationFrame(draw);

// === SPEECH RECOGNITION ===
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
const recognition = new SpeechRecognition();
recognition.lang = "ru-RU";
recognition.interimResults = false;
recognition.continuous = false;

let isSpeaking = false;

function listen() {
    if (isSpeaking) return;
    try {
        recognition.start();
        state = "listening";
        statusEl.innerText = "listening…";
    } catch {}
}

recognition.onresult = async e => {
    const text = e.results[0][0].transcript;
    transcriptEl.innerText = `You: ${text}`;
    await send(text);
};

recognition.onend = () => {
    if (!isSpeaking) setTimeout(listen, 800);
};

// === BACKEND ===
async function send(text) {
    isSpeaking = true;
    state = "thinking";
    statusEl.innerText = "generating…";

    try {
        const uid = tg.initDataUnsafe?.user?.id || 0;
        const r = await fetch(
            "https://patronal-mayme-unexpandable.ngrok-free.dev/api/voice_chat",
            {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    "ngrok-skip-browser-warning": "true"
                },
                body: JSON.stringify({ user_id: uid, text })
            }
        );

        const d = await r.json();
        transcriptEl.innerText += `\n\nBot: ${d.reply}`;
        speak(d.reply);

    } catch {
        statusEl.innerText = "connection lost";
        isSpeaking = false;
        listen();
    }
}

// === TTS ===
function speak(text) {
    state = "speaking";
    statusEl.innerText = "speaking…";

    const u = new SpeechSynthesisUtterance(
        text.replace(/<[^>]*>/g, "")
    );
    u.lang = "ru-RU";
    u.rate = 1.05;
    u.pitch = 0.95;

    u.onend = () => {
        isSpeaking = false;
        state = "idle";
        statusEl.innerText = "listening…";
        listen();
    };

    speechSynthesis.speak(u);
}

// === AUTOSTART ===
window.addEventListener("load", () => {
    setTimeout(listen, 1000);
});
</script>
</body>
</html>
