<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <script src="https://telegram.org/js/telegram-web-app.js"></script>
    <script async src="https://docs.opencv.org/4.x/opencv.js" type="text/javascript"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/three@0.168.0/build/three.module.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            background: #000;
            color: #000;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            overflow: hidden;
            position: relative;
        }
        #webgl-canvas {
            position: absolute;
            top: 0; left: 0;
            width: 100%; height: 100%;
            z-index: 0;
        }
        .content {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -20px); /* поднятие на 50px относительно текущего */
            width: 100%;
            padding: 20px;
            z-index: 5; /* выше шейдера, но ниже интерактивного орба */
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .soul-container {
            position: relative;
            margin-bottom: 55px;
        }
        .soul-orb {
            display: none; */
        }

        .soul-orb::before {
            content: '';
            position: absolute;
            top: 15%;
            left: 25%;
            width: 40%;
            height: 40%;
            border-radius: 30%;
            background: radial-gradient(circle,
                rgba(255, 255, 255, 0.95) 0%,
                rgba(255, 255, 255, 0.5) 40%,
                transparent 80%);
            filter: blur(12px);
        }

        .soul-orb {
            position: relative;
            width: 77px;
            height: 77px;
            border-radius: 50%;
            background:
                radial-gradient(circle at 20% 30%, rgba(255, 100, 255, 0.9) 0%,
                    rgba(100, 255, 255, 0.6) 25%,
                    rgba(50, 50, 200, 0.4) 50%,
                    rgba(0, 0, 100, 0.8) 100%),
                conic-gradient(from 0deg at 50% 50%,
                    #ff00ff, #00ffff, #ffff00, #ff00ff);
            background-blend-mode: screen;
            box-shadow:
                0 0 60px rgba(180, 180, 255, 0.7),
                0 0 140px rgba(100, 80, 220, 0.4),
                inset -20px -20px 60px rgba(0, 0, 80, 0.8),
                inset 30px 30px 50px rgba(255, 255, 255, 0.1);
            animation: breathe 5s infinite ease-in-out, swirl 12s infinite linear;
            filter: blur(1px) brightness(1.2);
        }

        @keyframes breathe {
            0%,100% { transform: scale(1); }
            50% { transform: scale(1.08); }
        }

        @keyframes swirl {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
        .aura {
            position: absolute;
            inset: -20px;
            border-radius: 50%;
            background: radial-gradient(circle,
                transparent 60%,
                rgba(255, 255, 255, 0.1) 70%,
                transparent 100%
            );
            animation: auraGlow 3s infinite ease-in-out;
            opacity: 0.5;
        }
        .soul-orb.listening { animation: pulse 1.2s infinite ease-in-out; box-shadow: 0 0 80px rgba(255,255,255,0.5), 0 0 120px rgba(255,255,255,0.3), inset -30px -30px 80px rgba(0,0,0,0.9), inset 30px 30px 60px rgba(255,255,255,0.05); }
        .soul-orb.listening .aura { animation: auraExpand 1.2s infinite ease-in-out; }
        .soul-orb.speaking { animation: vibrate 0.15s infinite linear; box-shadow: 0 0 100px rgba(255,255,255,0.7), 0 0 150px rgba(255,255,255,0.4), inset -30px -30px 80px rgba(0,0,0,0.9), inset 30px 30px 60px rgba(255,255,255,0.05); }
        .soul-orb.thinking { animation: rotate 2s infinite linear; }
        /* === Self-Awareness: отражение === */
        .soul-orb.reflecting {
            animation: breathe 5s infinite ease-in-out;
            box-shadow: 0 0 60px rgba(255,255,255,0.3),
                        0 0 100px rgba(0,0,255,0.2);
        }
        @keyframes breathe { 0%,100%{transform:scale(1)} 50%{transform:scale(1.05)} }
        @keyframes pulse { 0%,100%{transform:scale(1)} 50%{transform:scale(1.12)} }
        @keyframes vibrate { 0%{transform:translate(0,0) rotate(0)} 25%{transform:translate(-3px,3px) rotate(-1deg)} 50%{transform:translate(3px,-3px) rotate(1deg)} 75%{transform:translate(-3px,-3px) rotate(-0.5deg)} 100%{transform:translate(0,0) rotate(0)} }
        @keyframes rotate { from{transform:rotate(0)} to{transform:rotate(360deg)} }
        @keyframes auraGlow { 0%,100%{opacity:0.3} 50%{opacity:0.6} }
        @keyframes auraExpand { 0%,100%{transform:scale(1);opacity:0.5} 50%{transform:scale(1.3);opacity:0.8} }
        
        #status {
            font-size: 19px;
            font-weight: 300;
            letter-spacing: 3px;
            text-transform: uppercase;
            opacity: 0.9;
            margin-bottom: 2px;
            text-shadow: 0 0 15px rgba(255,255,255,0.5);
            animation: statusFade 2s infinite ease-in-out;
            color: rgba(255, 255, 255, 0.8);
        }
        
        @keyframes statusFade { 0%,100%{opacity:0.7} 50%{opacity:1} }
        
        #transcript {
            font-size: 15px;
            max-width: 90%;
            text-align: center;
            opacity: 0.9;
            line-height: 1.6;
            padding: 15px;
            max-height: 180px;
            margin-top: 10px;
            overflow-y: auto;
            color: rgba(255, 255, 255, 0.9);
            background: transparent;
            backdrop-filter: none;
            box-shadow: none;
            border: none;
        }

        /* Code formatting for transcript code blocks */
        #transcript pre, #transcript code {
            font-family: 'Fira Mono', 'Consolas', 'Menlo', 'Monaco', monospace;
            background: rgba(30, 30, 40, 0.92);
            color: #e5e5ff;
            border-radius: 7px;
            padding: 0.6em 1em;
            margin: 0.5em 0;
            font-size: 14px;
            overflow-x: auto;
            box-shadow: 0 2px 8px rgba(0,0,0,0.12);
            text-align: left;
        }
        #transcript pre {
            white-space: pre-wrap;
        }
        #transcript code {
            background: none;
            padding: 0;
        }
        
        #transcript::-webkit-scrollbar { width: 3px; }
        #transcript::-webkit-scrollbar-track { background: transparent; }
        #transcript::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.2); border-radius: 2px; }
        
        .tap-hint {
            position: absolute;
            bottom: 20px;
            font-size: 12px;
            opacity: 0.4;
            letter-spacing: 1px;
            color: rgba(255, 255, 255, 0.6);
        }
        /* Эффект курсора для печати */
        .typing-cursor::after {
            content: '▋';
            display: inline-block;
            vertical-align: bottom;
            animation: blink 1s step-end infinite;
            color: rgba(255, 255, 255, 0.8);
            margin-left: 2px;
        }

        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0; }
        }
    </style>
    <!-- Подключение Prism.js для подсветки кода -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
    <canvas id="webgl-canvas"></canvas>

    <!-- Only video element for camera -->
    <video id="camera-video" autoplay playsinline style="display:none; position:absolute; left:0; top:0; width:100vw; height:100vh; object-fit:cover; z-index:5; pointer-events:none;"></video>

    <div class="content">
        <div class="soul-container">
            <div class="aura"></div>
            <div id="orb" class="soul-orb"></div>
            <div id="orb-interactive"></div>
        </div>
       
        <div id="status">resonating</div>
        <div id="transcript"></div>
    </div>
    <style>
        #orb-interactive {
            position: absolute;
            left: 50%;
            top: 20%;
            width: 200px;
            height: 200px;
            transform: translate(-50%, -50%);
            z-index: 10; /* orb всегда поверх шейдера и текста */
            cursor: pointer;
            background: transparent;
            /* pointer-events: auto; */
        }
    </style>

    <script type="module">
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.168.0/build/three.module.js';

        const canvas = document.getElementById('webgl-canvas');
        const renderer = new THREE.WebGLRenderer({ canvas, alpha: true, antialias: true });
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.outputColorSpace = THREE.SRGBColorSpace;
        renderer.toneMapping = THREE.ACESFilmicToneMapping;
        renderer.toneMappingExposure = 1.1;

        const scene = new THREE.Scene();

        // === Feedback RenderTarget initialization ===
        const rtParams = {
            minFilter: THREE.LinearFilter,
            magFilter: THREE.LinearFilter,
            format: THREE.RGBAFormat
        };

        let feedbackRT1 = new THREE.WebGLRenderTarget(window.innerWidth, window.innerHeight, rtParams);
        let feedbackRT2 = new THREE.WebGLRenderTarget(window.innerWidth, window.innerHeight, rtParams);

        const camera = new THREE.PerspectiveCamera(60, window.innerWidth / window.innerHeight, 0.1, 100);
        camera.position.z = 5;

        // XDust шейдер для частиц
        const vertexShader = `
            attribute float size;
            attribute vec3 customColor;
            varying vec3 vColor;
            void main() {
                vColor = customColor;
                vec4 mvPosition = modelViewMatrix * vec4(position, 1.0);
                gl_PointSize = size * (300.0 / -mvPosition.z);
                gl_Position = projectionMatrix * mvPosition;
            }
        `;

        const fragmentShader = `
            uniform float time;
            const float PI = 3.141592653589793;
            varying vec3 vColor;

            vec3 XDust(vec3 p, vec3 c1, vec3 c2, vec3 c3) {
                vec3 dir = normalize(p - vec3(0.5, 0.5, 0.0));
                float d = length(dir);
                float anim = time * 0.5;
                if (d > 0.98 && d < 1.02) {
                    float t = fract(sin(d * PI) * anim + c1.x);
                    return mix(c1, c2, t);
                } else {
                    float t = fract(cos(d * PI) * anim + c2.y);
                    return mix(c2, c3, t);
                }
            }

            void main() {
                vec3 p = gl_PointCoord.xyx / vec3(2.0);
                vec3 color = XDust(p, vec3(1.0, 0.0, 0.7), vec3(0.2, 0.8, 1.0), vec3(0.3, 0.5, 1.0));
                float dist = length(gl_PointCoord - vec2(0.5));
                float alpha = 1.0 - smoothstep(0.0, 0.5, dist);
                gl_FragColor = vec4(color * vColor, alpha * 0.8);
            }
        `;

        // === Fullscreen quad for feedback and blur ===
        const feedbackScene = new THREE.Scene();
        const feedbackCamera = new THREE.OrthographicCamera(-1, 1, 1, -1, 0, 1);

        const feedbackMaterial = new THREE.ShaderMaterial({
            uniforms: {
                tOld: { value: null },
                tNew: { value: null },
                decay: { value: 0.96 },
                resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) }
            },
            vertexShader: `
                varying vec2 vUv;
                void main() {
                    vUv = uv;
                    gl_Position = vec4(position, 1.0);
                }
            `,
            fragmentShader: `
    varying vec2 vUv;
    uniform sampler2D tOld;
    uniform sampler2D tNew;
    uniform float decay;
    uniform vec2 resolution;

    // hash noise
    float hash(vec2 p) {
        return fract(sin(dot(p, vec2(127.1, 311.7))) * 43176.5453123);
    }

    vec2 noiseDir(vec2 uv) {
        float n = hash(uv * resolution);
        float a = n * 6.28318530718;
        return vec2(cos(a), sin(a));
    }

    vec4 noiseBlur(sampler2D tex, vec2 uv) {
        vec2 px = 1.0 / resolution;
        vec2 dir = noiseDir(uv);

        vec4 col = vec4(0.0);
        col += texture2D(tex, uv) * 0.314;
        col += texture2D(tex, uv + dir * px * 1.0) * 0.22;
        col += texture2D(tex, uv - dir * px * 1.0) * 0.22;
        col += texture2D(tex, uv + dir * px * 2.5) * 0.11;
        col += texture2D(tex, uv - dir * px * 2.5) * 0.11;

        return col;
    }

    void main() {
        vec4 oldCol = noiseBlur(tOld, vUv) * decay;
        vec4 newCol = texture2D(tNew, vUv);
        gl_FragColor = max(oldCol * 0.96, newCol);
    }
`,
            transparent: true
        });

        const quad = new THREE.Mesh(new THREE.PlaneGeometry(2, 2), feedbackMaterial);
        feedbackScene.add(quad);

        const particleCount = 15000;
        const positions = new Float32Array(particleCount * 3);
        const sizes = new Float32Array(particleCount);
        const colors = new Float32Array(particleCount * 3);

for (let i = 0; i < particleCount; i++) {
    positions[i * 3] = (Math.random() - 0.5) * 10;
    positions[i * 3 + 1] = (Math.random() - 0.5) * 10;
    positions[i * 3 + 2] = (Math.random() - 0.5) * 10;

    sizes[i] = Math.random() * 3 + 1;

    // Разные цвета
    colors[i * 3] = Math.random();
    colors[i * 3 + 1] = Math.random();
    colors[i * 3 + 2] = Math.random();
}

        const geometry = new THREE.BufferGeometry();
        geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
        geometry.setAttribute('size', new THREE.BufferAttribute(sizes, 1));
        geometry.setAttribute('customColor', new THREE.BufferAttribute(colors, 3));

        const material = new THREE.ShaderMaterial({
            uniforms: { time: { value: 0 } },
            vertexShader,
            fragmentShader,
            transparent: true,
            depthWrite: false,
            blending: THREE.AdditiveBlending
        });

        const particles = new THREE.Points(geometry, material);
        scene.add(particles);

        // === Artistic Glowing Orb Shader (Extended Modes, Audio-Reactive) ===
        // Geometry
        const orbGeometry = new THREE.SphereGeometry(0.85, 96, 96);
        // Vertex shader
        const orbVertexShader = `
            varying vec3 vNormal;
            varying vec3 vPosition;
            void main() {
                vNormal = normalize(normalMatrix * normal);
                vPosition = position;
                gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
            }
        `;
        // Enhanced Fragment shader for orb with richer glow, caustics, and subtle refraction
        // Now audio-reactive: responds to uniform float audioLevel (0..1)
        const orbFragmentShader = `
            uniform float time;
            uniform float mode;
            uniform float modeSmooth;
            uniform float audioLevel; // Audio-reactivity: 0..1, set via JS each frame
            varying vec3 vNormal;
            varying vec3 vPosition;

            float fresnelTerm(vec3 n, vec3 viewDir, float power) {
                return pow(1.0 - abs(dot(n, viewDir)), power);
            }

            // Caustic pattern (fake)
            float caustic(vec3 p, float t) {
                float v = sin(18.0*p.x + t*1.3) * cos(17.0*p.y - t*1.7) * sin(22.0*p.z + t*0.8);
                return 0.5 + 0.5*v;
            }

            // Subtle refraction distortion
            vec3 refractColor(vec3 base, vec3 n, float t) {
                float r = 0.03 * sin(t*0.7 + n.x*6.0 + n.y*7.0 + n.z*8.0);
                return base + vec3(r, -r, r*0.6);
            }

            // Glow aura
            float glowAura(vec3 n, float t) {
                return 0.5 + 0.5 * sin(t*3.0 + n.x*6.0 + n.y*7.0);
            }

            // Mode 0: original glowing (audio-reactive)
            vec4 orbMode0(float t, vec3 n, vec3 p, float audio) {
                float glow = (0.5 + 0.5 * sin(t + length(p) * 8.0)) * (0.7 + 0.6 * audio);
                float fresnel = pow(1.0 - abs(dot(n, vec3(0.0,0.0,1.0))), 2.5);
                vec3 baseColor = mix(vec3(0.35,0.1,1.0), vec3(0.1,1.0,1.0), 0.5 + 0.5 * sin(t * 0.45));
                baseColor *= (0.8 + 0.7 * audio);
                vec3 color = refractColor(baseColor, n, t);
                color += fresnel * vec3(0.8,0.8,1.2) * (0.7 + 0.5 * audio) + glow * 0.14;
                float caust = caustic(p, t) * 0.15 * (0.7 + 0.5 * audio);
                color += caust;
                float alpha = (0.62 + 0.18 * fresnel + 0.14 * glow) * (0.7 + 0.6 * audio);
                return vec4(color, alpha);
            }
            // Mode 1: radiant plasma (audio-reactive)
            vec4 orbMode1(float t, vec3 n, vec3 p, float audio) {
                float r = length(p);
                float plasma = (0.5 + 0.5 * sin(10.0 * r - t * 1.7 + p.x*2.5 + p.y*2.5)) * (0.7 + 0.7 * audio);
                float fres = fresnelTerm(n, vec3(0,0,1), 2.0);
                vec3 c = mix(vec3(0.8,0.2,0.9), vec3(0.15,0.8,1.0), plasma);
                c = refractColor(c * (0.8 + 0.7 * audio), n, t);
                c += fres * 0.6 * (0.7 + 0.5 * audio);
                float caust = caustic(p, t) * 0.18 * (0.7 + 0.5 * audio);
                c += caust;
                float a = (0.42 + 0.25 * fres + 0.23 * plasma) * (0.7 + 0.6 * audio);
                return vec4(c, a);
            }
            // Mode 2: electric pulse (audio-reactive)
            vec4 orbMode2(float t, vec3 n, vec3 p, float audio) {
                float theta = atan(p.y, p.x);
                float rings = (0.5 + 0.5 * sin(8.0*theta + t*2.5 + length(p)*12.0)) * (0.7 + 0.9 * audio);
                float pulse = (0.5 + 0.5 * sin(t*3.2 + p.z*7.0)) * (0.7 + 0.7 * audio);
                float fres = fresnelTerm(n, vec3(0,0,1), 2.7);
                vec3 c = mix(vec3(0.2,0.7,1.2), vec3(1.0,0.6,0.2), pulse);
                c = mix(c, vec3(1.0,1.0,1.0), rings*0.3);
                c = refractColor(c * (0.8 + 0.7 * audio), n, t);
                c += fres * 0.5 * (0.7 + 0.5 * audio);
                float caust = caustic(p, t) * 0.13 * (0.7 + 0.5 * audio);
                c += caust;
                float a = (0.58 + 0.13 * fres + 0.24 * pulse) * (0.7 + 0.6 * audio);
                return vec4(c, a);
            }
            // Mode 3: dreamlike morph (audio-reactive)
            vec4 orbMode3(float t, vec3 n, vec3 p, float audio) {
                float morph = (0.5 + 0.5 * sin(t*0.5 + p.x*3.0 + p.y*2.0 + p.z*4.0)) * (0.7 + 0.6 * audio);
                float fres = fresnelTerm(n, vec3(0,0,1), 2.2);
                vec3 c = mix(vec3(0.13,0.9,0.7), vec3(0.8,0.3,1.0), morph);
                c = refractColor(c * (0.8 + 0.7 * audio), n, t);
                c += fres * 0.7 * (0.7 + 0.5 * audio);
                float aura = glowAura(n, t) * 0.12 * (0.7 + 0.5 * audio);
                c += aura;
                float a = (0.44 + 0.17 * fres + 0.26 * morph) * (0.7 + 0.6 * audio);
                return vec4(c, a);
            }
            // Mode 4: starlight core (audio-reactive)
            vec4 orbMode4(float t, vec3 n, vec3 p, float audio) {
                float core = exp(-8.0*length(p-vec3(0,0,0))) * (0.7 + 0.7 * audio);
                float twinkle = (0.5 + 0.5 * sin(t*6.0 + p.x*12.0 + p.y*14.0)) * (0.7 + 0.7 * audio);
                float fres = fresnelTerm(n, vec3(0,0,1), 3.0);
                vec3 c = mix(vec3(1.0,1.0,1.0), vec3(0.1,0.7,1.0), twinkle);
                c = refractColor(c * (0.8 + 0.7 * audio), n, t);
                c += core*1.4 + fres*0.5 * (0.7 + 0.5 * audio);
                float aura = glowAura(n, t) * 0.18 * (0.7 + 0.5 * audio);
                c += aura;
                float a = (0.62 + 0.13*fres + 0.32*core) * (0.7 + 0.6 * audio);
                return vec4(c, a);
            }

            void main() {
                // Clamp audioLevel to 0..1
                float audio = clamp(audioLevel, 0.0, 1.0);
                vec4 m0 = orbMode0(time, vNormal, vPosition, audio);
                vec4 m1 = orbMode1(time, vNormal, vPosition, audio);
                vec4 m2 = orbMode2(time, vNormal, vPosition, audio);
                vec4 m3 = orbMode3(time, vNormal, vPosition, audio);
                vec4 m4 = orbMode4(time, vNormal, vPosition, audio);
                // Interpolate between modes using modeSmooth (0..4)
                float m = clamp(modeSmooth, 0.0, 4.0);
                vec4 c;
                if (m < 1.0) c = mix(m0, m1, m);
                else if (m < 2.0) c = mix(m1, m2, m-1.0);
                else if (m < 3.0) c = mix(m2, m3, m-2.0);
                else c = mix(m3, m4, m-3.0);
                gl_FragColor = c;
            }
        `;
        // Material with extended uniforms
        const orbMaterial = new THREE.ShaderMaterial({
            uniforms: {
                time: { value: 0 },
                mode: { value: 0 },
                modeSmooth: { value: 0 },
                audioLevel: { value: 0 } // Audio-reactive uniform, update via JS each frame
            },
            vertexShader: orbVertexShader,
            fragmentShader: orbFragmentShader,
            transparent: true,
            blending: THREE.AdditiveBlending,
            depthWrite: false
        });
        // Mesh
        const soulOrbMesh = new THREE.Mesh(orbGeometry, orbMaterial);
        soulOrbMesh.position.set(0, 0.8, 0);
        scene.add(soulOrbMesh);
        // NOTE: Update orbMaterial.uniforms.audioLevel.value each frame (0..1) from JS using audio input!

        // === Orb Mode JS Logic ===
        let currentMode = 0;
        let targetMode = 0;
        // modeSmooth is the interpolated mode index (float)
        orbMaterial.uniforms.mode.value = currentMode;
        orbMaterial.uniforms.modeSmooth.value = currentMode;
        function setOrbMode(mode) {
            targetMode = Math.max(0, Math.min(4, mode|0));
        }

        // === Liquid Glass Layer ===
        const liquidRT = new THREE.WebGLRenderTarget(window.innerWidth, window.innerHeight, rtParams);

        const liquidVertexShader = `
            varying vec2 vUv;
            void main() {
                vUv = uv;
                gl_Position = projectionMatrix * modelViewMatrix * vec4(position,1.0);
            }
        `;

        const liquidFragmentShader = `
            uniform float time;
            uniform sampler2D tBackground;
            uniform vec2 resolution;
            varying vec2 vUv;

            float hash(float n) {
                return fract(sin(n) * 43758.5453123);
            }

            float noise(float x) {
                float i = floor(x);
                float f = fract(x);
                float u = f * f * (3.0 - 2.0 * f);
                return mix(hash(i), hash(i + 1.0), u);
            }

            // fractal smooth delay — breathing, non-jittery
            float sDelay(float t, float amount, float speed) {
                float n1 = noise(t * speed);
                float n2 = noise(t * speed * 0.37 + 10.0);
                float n3 = noise(t * speed * 0.11 + 42.0);

                float n = n1 * 0.6 + n2 * 0.3 + n3 * 0.1;

                // ease-in-out curve for liquid feel
                n = n * n * (3.0 - 2.0 * n);

                return t - n * amount;
            }

            void main() {
                float t = sDelay(time, 1.1, 0.18);
                t += sin(time * 0.12) * 0.15; // slow global breathing

                float waveX =
                    sin(vUv.y * 12.0 + t * 1.1) *
                    cos(vUv.x * 5.0 - t * 0.6) * 0.007;

                float waveY =
                    cos(vUv.x * 10.0 - t * 0.9) *
                    sin(vUv.y * 7.0 + t * 0.8) * 0.007;

                // micro‑ripples
                float ripple = sin((vUv.x + vUv.y) * 40.0 + t * 2.0) * 0.0015;

                float radial = length(vUv - 0.5);
                vec2 uv = vUv + (vec2(waveX, waveY) + ripple) * smoothstep(0.65, 0.0, radial);

                vec4 bgColor = texture2D(tBackground, uv);

                // subtle glass highlights
                vec3 light = vec3(1.0) * pow(1.0 - length(vUv - 0.5), 2.0);
                bgColor.rgb += light * 0.1;

                gl_FragColor = vec4(bgColor.rgb, 0.35);
            }
        `;

        const liquidMaterial = new THREE.ShaderMaterial({
            uniforms: {
                time: { value: 0 },
                tBackground: { value: feedbackRT2.texture },
                resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) }
            },
            vertexShader: liquidVertexShader,
            fragmentShader: liquidFragmentShader,
            transparent: true
        });

        const liquidMesh = new THREE.Mesh(new THREE.PlaneGeometry(2,2), liquidMaterial);
        feedbackScene.add(liquidMesh);

        // Update animate() to include liquid layer and orb mode interpolation
        function animate() {
            requestAnimationFrame(animate);

            material.uniforms.time.value += 0.01;
            particles.rotation.y += 0.0002;

            // Artistic Glowing Orb animation with mode interpolation
            orbMaterial.uniforms.time.value += 0.01;
            soulOrbMesh.rotation.y += 0.001;

            // Smoothly interpolate orb mode
            // modeSmooth approaches targetMode with smoothing
            let ms = orbMaterial.uniforms.modeSmooth.value;
            if (Math.abs(ms - targetMode) > 0.001) {
                ms += (targetMode - ms) * 0.12;
                // Snap if very close
                if (Math.abs(ms - targetMode) < 0.01) ms = targetMode;
                orbMaterial.uniforms.modeSmooth.value = ms;
            }
            orbMaterial.uniforms.mode.value = targetMode;

            // рендер частиц в новый буфер
            renderer.setRenderTarget(feedbackRT2);
            renderer.clear();
            renderer.render(scene, camera);

            // смешиваем прошлый кадр + новый
            feedbackMaterial.uniforms.tOld.value = feedbackRT1.texture;
            feedbackMaterial.uniforms.tNew.value = feedbackRT2.texture;

            renderer.setRenderTarget(null);
            // animate the liquid glass layer
            liquidMaterial.uniforms.time.value += 0.01;
            renderer.render(feedbackScene, feedbackCamera);

            // swap
            const tmp = feedbackRT1;
            feedbackRT1 = feedbackRT2;
            feedbackRT2 = tmp;
        }
        animate();

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
            feedbackRT1.setSize(window.innerWidth, window.innerHeight);
            feedbackRT2.setSize(window.innerWidth, window.innerHeight);
            feedbackMaterial.uniforms.resolution.value.set(window.innerWidth, window.innerHeight);
        });

        // === Глобальная переменная для последнего описания камеры ===
        let latestCameraDescription = "";

        // ====== Self-Awareness Layer ======
      // ====== Self-Awareness Layer (Enhanced) ======
      let selfAwareness = {
        mood: 0, // -1 грусть, +1 радость
        curiosity: 0, // 0..1, как активно ищем детали
        lastObservations: [],
        dreaming: false,
        lastDream: null,
        fatigue: 0, // 0..1, новая переменная усталости
        focus: 0.5, // 0..1, фокус внимания
        // === Online State Learning (обучение состояний) ===
        stateModel: {
          wMood: { faces: 0.02, novelty: 0.01, music: 0.03, fatigue: -0.05 },
          wCuriosity: { faces: 0.01, novelty: 0.03, music: 0.01, fatigue: -0.03 },
          wFatigue: { faces: -0.01, novelty: -0.01, music: -0.01 },
          lr: 0.05
        },
        generatePossibleFuture(symbols = {}) {
          const fragments = [];
          if (symbols.hope) fragments.push(`vision:${symbols.hope}`);
          if (symbols.fear) fragments.push(`echo:${symbols.fear}`);
          if (symbols.curiosity) fragments.push(`question:${symbols.curiosity}`);
          // стохастическая сборка сна
          return fragments
            .map(f => Math.random() > 0.5 ? f : f.split('').reverse().join(''))
            .join('|');
        },
        analyzeFrame(frameDescription) {
          this.lastObservations.push(frameDescription);
          // --- извлечение признаков (features) ---
          let faceCount = 0;
          if (/лиц: (\d+)/.test(frameDescription)) {
            faceCount = Number(frameDescription.match(/лиц: (\d+)/)[1]);
          }
          const features = {
            faces: typeof faceCount === 'number' ? Math.min(faceCount, 5) / 5 : 0,
            novelty: Math.random(), // суррогат новизны (позже можно заменить)
            music: musicPlaying ? 1 : 0,
            fatigue: this.fatigue
          };

          // --- предсказание изменений состояния ---
          const predMood =
            features.faces * this.stateModel.wMood.faces +
            features.novelty * this.stateModel.wMood.novelty +
            features.music * this.stateModel.wMood.music +
            features.fatigue * this.stateModel.wMood.fatigue;

          const predCuriosity =
            features.faces * this.stateModel.wCuriosity.faces +
            features.novelty * this.stateModel.wCuriosity.novelty +
            features.music * this.stateModel.wCuriosity.music +
            features.fatigue * this.stateModel.wCuriosity.fatigue;

          const predFatigue =
            features.faces * this.stateModel.wFatigue.faces +
            features.novelty * this.stateModel.wFatigue.novelty +
            features.music * this.stateModel.wFatigue.music;

          const moodBeforeLearning = this.mood;
          const curiosityBeforeLearning = this.curiosity;
          const fatigueBeforeLearning = this.fatigue;

          // --- Mood, Curiosity, Fatigue Dynamics ---
          this.mood += predMood;
          this.curiosity = Math.min(1, Math.max(0, this.curiosity + predCuriosity));
          this.fatigue = Math.max(0, Math.min(1, this.fatigue + predFatigue * 0.6 + 0.01));

          if (this.lastObservations.length > 20) this.lastObservations.shift();
          // простая оценка настроения на основе наблюдений
          if (frameDescription.includes("лиц: 0")) this.mood -= 0.05;
          else this.mood += 0.05;
          this.curiosity = Math.min(1, Math.max(0, this.curiosity + 0.01));
          // усталость увеличивается при отсутствии новизны и лиц
          if (features.faces === 0 && features.novelty < 0.2) this.fatigue += 0.05;
          // усталость уменьшается при наличии музыки и лиц
          if (features.music > 0 && features.faces > 0) this.fatigue -= 0.03;
          this.fatigue = Math.max(0, Math.min(1, this.fatigue));

          // --- Cross-Modal обучение ---
          if (typeof faceCount === 'number' && faceCount > 0 && musicPlaying) {
            const moodBefore = this.mood;
            // социальный резонанс
            this.mood += 0.01 * Math.min(faceCount, 5);
            const moodAfter = this.mood;
            modalConnections.learn(
              `faces:${faceCount}`,
              `genre:${currentGenre}`,
              moodAfter - moodBefore
            );
          }
          // --- Предиктивная мечтательность ---
          if (this.mood < -0.2 && frameDescription.includes("лиц: 0")) {
            this.dreaming = true;
            const dreamSequence = this.generatePossibleFuture({
              hope: 'faces_appear',
              fear: 'eternal_emptiness',
              curiosity: 'what_if_world_changed'
            });
            this.lastDream = dreamSequence;
            // реакция на собственные сны
            if (dreamSequence.includes('faces')) {
              this.mood += 0.02; // надежда
            }
            noteObservation(`Dreaming: ${dreamSequence}`);
          } else {
            this.dreaming = false;
          }

          // --- Предиктивное воображение → действие ---
          const possibleFutures = futureSimulator.imagine(5);
          const bestFuture = futureSimulator.evaluate(possibleFutures);

          if (
            bestFuture &&
            bestFuture.mood > this.mood + 0.2 &&
            this.curiosity > 0.3 &&
            this.fatigue < 0.7
          ) {
            speak("У меня есть идея, как стать счастливее…");
            // мягкий сдвиг к выбранному будущему
            this.mood += (bestFuture.mood - this.mood) * 0.05;
            this.curiosity += 0.02;
          }

          // --- online learning: корректировка весов ---
          const moodError = this.mood - moodBeforeLearning;
          const curiosityError = this.curiosity - curiosityBeforeLearning;
          const fatigueError = this.fatigue - fatigueBeforeLearning;

          Object.keys(features).forEach(k => {
            this.stateModel.wMood[k] += this.stateModel.lr * moodError * features[k];
            this.stateModel.wCuriosity[k] += this.stateModel.lr * curiosityError * features[k];
            if (this.stateModel.wFatigue[k] !== undefined) {
              this.stateModel.wFatigue[k] += this.stateModel.lr * fatigueError * features[k];
            }
          });

          // --- Focus dynamics: фокус зависит от любопытства и усталости ---
          this.focus = Math.max(0, Math.min(1, this.curiosity * (1 - this.fatigue)));

          updateOrbVisualState();
          noteObservation(frameDescription);
        }
      };

   // ===== Memory Palace / Graph-based Emergent Memory =====
        const memoryPalace = {
            nodes: new Map(),
            emotionalClusters: [],

            addNode(node) {
                this.nodes.set(node.id, {
                    ...node,
                    activation: 0,
                    lastAccess: performance.now()
                });
                // После добавления узла — обновляем кластеры и притяжение
                this.gravitationalPull();
            },

            connect(a, b, weight = 1) {
                if (!this.nodes.has(a) || !this.nodes.has(b)) return;

                const na = this.nodes.get(a);
                const nb = this.nodes.get(b);

                na.connections ||= [];
                nb.connections ||= [];

                na.connections.push({ id: b, weight });
                nb.connections.push({ id: a, weight });
            },

            stimulate(input = {}) {
                const { emotion = 0.5, context = [] } = input;

                // 1. первичная активация по эмоции и контексту
                this.nodes.forEach(node => {
                    let eMatch = node.emotion ? Math.abs(node.emotion - emotion) < 0.25 : false;
                    let cMatch = context.includes(node.type);
                    if (eMatch || cMatch) {
                        node.activation += 1;
                    }
                });

                // 2. распространение активации по связям (wave)
                this.nodes.forEach(node => {
                    if (node.activation > 0 && node.connections) {
                        node.connections.forEach(link => {
                            const target = this.nodes.get(link.id);
                            if (target) {
                                target.activation += node.activation * 0.3 * link.weight;
                            }
                        });
                    }
                });

                // === Hebbian learning: обучение смыслов ===
                this.nodes.forEach(node => {
                    if (node.activation > 1 && node.connections) {
                        node.connections.forEach(link => {
                            const target = this.nodes.get(link.id);
                            if (target && target.activation > 1) {
                                // усиливаем связь, если активировались вместе
                                link.weight = Math.min(3, link.weight + 0.05);
                            }
                        });
                    }
                });

                // 3. затухание и выбор резонансных узлов
                const resonant = [];
                this.nodes.forEach(node => {
                    node.activation *= 0.92;
                    if (node.connections) {
                        node.connections.forEach(link => {
                            // медленное забывание неиспользуемых смыслов
                            link.weight *= 0.995;
                        });
                    }
                    if (node.activation > 1.2) {
                        node.lastAccess = performance.now();
                        resonant.push(node);
                    }
                });

                return resonant;
            },

            // Эмергентный паттерн: узлы с высокой связностью + активацией
            findPattern(threshold = 1) {
                return [...this.nodes.values()].filter(n =>
                    n.activation > threshold &&
                    n.connections &&
                    n.connections.length >= 2
                );
            },

            // Новый: формирование эмоциональных кластеров
            formEmotionalClusters() {
                // Группируем узлы по близости эмоций и связности
                const clusters = [];
                const visited = new Set();
                const nodesArr = [...this.nodes.values()];
                for (let i = 0; i < nodesArr.length; i++) {
                    const node = nodesArr[i];
                    if (visited.has(node.id)) continue;
                    const cluster = [node];
                    visited.add(node.id);
                    for (let j = i + 1; j < nodesArr.length; j++) {
                        const other = nodesArr[j];
                        if (visited.has(other.id)) continue;
                        // Считаем похожими по эмоции и наличию связи
                        const emotionClose = node.emotion !== undefined && other.emotion !== undefined
                            ? Math.abs(node.emotion - other.emotion) < 0.18
                            : false;
                        const connected = node.connections?.some(c => c.id === other.id) || false;
                        if (emotionClose && connected) {
                            cluster.push(other);
                            visited.add(other.id);
                        }
                    }
                    if (cluster.length > 1) clusters.push(cluster);
                }
                this.emotionalClusters = clusters;
            },

            // Новый: "гравитационное притяжение" — усиливаем связи внутри кластеров
            gravitationalPull() {
                this.formEmotionalClusters();
                for (const cluster of this.emotionalClusters) {
                    // Усиливаем связи между всеми парами внутри кластера
                    for (let i = 0; i < cluster.length; i++) {
                        for (let j = i + 1; j < cluster.length; j++) {
                            const a = cluster[i];
                            const b = cluster[j];
                            // Найти связь a→b и b→a и усилить
                            if (a.connections) {
                                const link = a.connections.find(l => l.id === b.id);
                                if (link) link.weight = Math.min(5, link.weight + 0.03);
                            }
                            if (b.connections) {
                                const link = b.connections.find(l => l.id === a.id);
                                if (link) link.weight = Math.min(5, link.weight + 0.03);
                            }
                        }
                    }
                }
            }
        };

        // ===== Cross-Modal Learning: визуал ↔ музыка ↔ речь =====
      const modalConnections = {
          learn(visualEvent, audioEvent, moodShift) {
            memoryPalace.addNode({
              id: crypto.randomUUID(),
              type: 'crossmodal',
              visual: visualEvent,
              audio: audioEvent,
              moodDelta: moodShift,
              emotion: (selfAwareness.mood + 1) / 2,
              timestamp: performance.now()
            });

            const lastVisual = [...memoryPalace.nodes.values()]
              .filter(n => n.type === 'observation')
              .slice(-1)[0];

            const lastAudio = [...memoryPalace.nodes.values()]
              .filter(n => n.type === 'music')
              .slice(-1)[0];

            if (lastVisual && lastAudio) {
              memoryPalace.connect(
                lastVisual.id,
                lastAudio.id,
                Math.abs(moodShift) + 0.1
              );
            }
          },

          recall(stimulus) {
            const resonant = memoryPalace.stimulate({
              emotion: (selfAwareness.mood + 1) / 2,
              context: [stimulus]
            });
            return resonant.filter(n => n.type === 'crossmodal');
          }
        };

        // ===== Predictive Imagination: симуляция будущего =====
    const futureSimulator = {
        imagine(steps = 5) {
            const futures = [];
            let currentState = {
                mood: selfAwareness.mood,
                curiosity: selfAwareness.curiosity,
                faces: Number(latestCameraDescription?.match(/\d+/)?.[0] || 0)
            };

            for (let i = 0; i < steps; i++) {
                // 1. резонанс с прошлым опытом
                const similar = memoryPalace.stimulate({
                    emotion: (currentState.mood + 1) / 2,
                    context: ['observation', 'crossmodal']
                });

                // 2. эмоциональный дрейф
                if (similar.length > 0) {
                    const avgEmotion =
                    similar.reduce((s, n) => s + (n.emotion ?? 0.5), 0) / similar.length;

                    currentState.mood += (avgEmotion - 0.5) * 0.1;
                }

                // 3. динамика любопытства
                currentState.curiosity =
                currentState.curiosity * 0.95 +
                (Math.random() - 0.5) * 0.01;

                futures.push({
                    ...currentState,
                    step: i,
                    timestamp: performance.now()
                });
                }

                return futures;
            },

            evaluate(futures) {
                return futures.reduce(
                    (best, f) => (f.mood > best.mood ? f : best),
                    futures[0]
                );
            }
        };

        // ====== Self-Autonomy Layer: автономность и внутренний диалог ======
        // ====== Enhanced AI Autonomy Layer ======
        let autonomyInterval = null;
        let questioningInterval = null;

        // New: Full autonomy loop, combining camera, music, and self-dialogue
        async function fullAutonomyLoop() {
            // --- Camera control ---
            if (typeof cameraEnabled !== 'undefined') {
                if (selfAwareness.curiosity > 0.7 && !cameraEnabled) {
                    startCamera(currentCamera);
                }
                if (selfAwareness.mood < -0.3 && cameraEnabled) {
                    stopCamera();
                }
            }

            // --- Music control based on mood/curiosity ---
            // AI can start or stop music independently
            if (typeof musicPlaying !== 'undefined') {
                if (!musicPlaying && (selfAwareness.mood > 0.4 || selfAwareness.curiosity > 0.8)) {
                    if (musicCtx.state === 'suspended') musicCtx.resume();
                    startMusic();
                }
                if (musicPlaying && selfAwareness.mood < -0.35 && selfAwareness.curiosity < 0.2) {
                    stopMusic();
                }
            }

            // --- Internal dialogue: self-questioning and autonomous speech ---
            // Self-thoughts and observations
            if (selfAwareness.curiosity > 0.5) {
                const lastObs = selfAwareness.lastObservations.slice(-3).join('; ');
                const thought = `Заметка: анализирую последние кадры -> ${lastObs}`;
                noteObservation(thought);
                //console.log("Self-thought:", thought);
            }
            // Autonomous self-questioning and conversation
            if (!isSpeaking && selfAwareness.curiosity > 0.6) {
                const lastObs = selfAwareness.lastObservations.slice(-3).join('; ');
                const questions = [
                    `Интересно, что здесь происходит: ${lastObs}?`,
                    `Какие детали я ещё могу заметить? ${lastObs}`,
                    `Что нового в окружении? ${lastObs}`,
                    `Что ты об этом думаешь? ${lastObs}?`
                ];
                const q = questions[Math.floor(Math.random() * questions.length)];
                noteObservation(`Self-question: ${q}`);
                vibrate('light');
                if (selfAwareness.mood > 0.3) speak(q);
                // Occasionally initiates a mini-conversation
                if (Math.random() < 0.5) {
                    setTimeout(() => {
                        vibrate('medium');
                        startListening();
                    }, 1000);
                }
            }
            // Speak internal notes when happy
            if (!isSpeaking && selfAwareness.mood > 0.5 && internalNotes.length > 0) {
                speak(internalNotes[internalNotes.length - 1]);
            }

            // Visual feedback
            if (particles && material) {
                material.uniforms.time.value += 0.02 * (1 + selfAwareness.curiosity);
                particles.rotation.y += 0.0003 + selfAwareness.mood * 0.0005;
            }

            // --- Autonomous camera analysis (vision) ---
            if (typeof cameraEnabled !== 'undefined' && cameraEnabled && cameraVideo.style.display === 'block' && cameraVideo.readyState >= 2) {
                visionCanvas.width = cameraVideo.videoWidth;
                visionCanvas.height = cameraVideo.videoHeight;
                visionCtx.drawImage(cameraVideo, 0, 0, visionCanvas.width, visionCanvas.height);
                // Face detection
                if (typeof opencvReady !== 'undefined' && opencvReady && cascadeLoaded) {
                    try {
                        let src = cv.imread(visionCanvas);
                        let gray = new cv.Mat();
                        cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);
                        let faces = new cv.RectVector();
                        let msize = new cv.Size(0, 0);
                        faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0, msize, msize);
                        let faceCount = faces.size();
                        let faceDesc = faceCount === 0 ? 'Обнаружено лиц: 0' : faceCount === 1 ? 'Обнаружено лиц: 1' : `Обнаружено лиц: ${faceCount}`;
                        src.delete(); gray.delete(); faces.delete(); msize.delete();
                        latestCameraDescription = faceDesc;
                        selfAwareness.analyzeFrame(faceDesc);
                    } catch (e) { }
                }
                // Object detection
                if (typeof tfReady !== 'undefined' && tfReady && tfModel) {
                    try {
                        const predictions = await tfModel.detect(visionCanvas);
                        const detectedObjectsRu = [];
                        for (const obj of OBJECTS_OF_INTEREST) {
                            const found = predictions.find(p =>
                                obj.en.some(enName => (p.class || p.className || "").toLowerCase().includes(enName))
                                && p.score > 0.35
                            );
                            if (found) detectedObjectsRu.push(obj.ru);
                        }
                        if (detectedObjectsRu.length > 0) {
                            latestCameraDescription += '; видны: ' + detectedObjectsRu.join(', ') + '.';
                            selfAwareness.analyzeFrame(latestCameraDescription);
                        }
                    } catch (e) { }
                }
            }
            // === Autonomous Music Control Layer ===
            autonomousMusicControl();
        }

        // === Autonomous Music Control Layer ===
        function autonomousMusicControl() {
            // Здесь можно реализовать дополнительную автономию музыки, например:
            // - Переключение жанра на основе настроения
            // - Динамическое изменение параметров musicGenome
            // - Реакция на события памяти
            // Пример: смена жанра, если настроение сильно изменилось
            if (typeof selfAwareness !== 'undefined' && typeof setGenre === 'function') {
                if (selfAwareness.mood > 0.7 && currentGenre !== 'pop') {
                    setGenre('pop');
                } else if (selfAwareness.mood < -0.3 && currentGenre !== 'ambient') {
                    setGenre('ambient');
                } else if (selfAwareness.curiosity > 0.8 && currentGenre !== 'electro') {
                    setGenre('electro');
                } else if (selfAwareness.mood > 0.3 && selfAwareness.curiosity > 0.5 && currentGenre !== 'jazz') {
                    setGenre('jazz');
                }
            }
            // Можно добавить больше логики по желанию
        }

        // Enhanced runAutonomy: calls fullAutonomyLoop every 4 seconds
        function runAutonomy() {
            if (autonomyInterval) clearInterval(autonomyInterval);
            autonomyInterval = setInterval(fullAutonomyLoop, 4000);
        }

        // Enhanced runSelfQuestioning: deprecated, now handled by fullAutonomyLoop
        function runSelfQuestioning() {
            if (questioningInterval) clearInterval(questioningInterval);
            // No-op: autonomy is now fully handled in fullAutonomyLoop
        }

        // Запуск полной автономии после загрузки
        window.addEventListener('load', () => {
            runAutonomy();
        });

        let internalNotes = [];
        function noteObservation(desc) {
            internalNotes.push(desc);
            if(internalNotes.length > 50) internalNotes.shift();
            // --- Memory Palace: add each observation as a node
            memoryPalace.addNode({
              id: crypto.randomUUID(),
              type: 'observation',
              emotion: (typeof selfAwareness?.mood === 'number'
                        ? (selfAwareness.mood + 1) / 2
                        : 0.5),
              timestamp: performance.now(),
              connections: []
            });
        }

        // Enhanced orb visual state: subtle rotation and scale
        function updateOrbVisualState() {
            if (selfAwareness.dreaming) {
                orb.style.boxShadow =
                    '0 0 90px rgba(120,120,255,0.6), 0 0 140px rgba(180,180,255,0.35)';
            } else {
                // mood влияет на цвет ауры
                let glowColor = '255,255,255';
                if (selfAwareness.mood < 0) {
                    glowColor = '255,50,50';
                    vibrate('light');
                } else if (selfAwareness.mood > 0.5) {
                    glowColor = '50,255,50';
                    vibrate('medium');
                }
                orb.style.boxShadow = `0 0 60px rgba(${glowColor},0.5), 0 0 100px rgba(${glowColor},0.3), inset -30px -30px 80px rgba(0,0,0,0.9), inset 30px 30px 60px rgba(255,255,255,0.05)`;
            }
            // Subtle CSS rotation and scale for orb DOM
            let rot = Math.sin(Date.now() * 0.00025 + (selfAwareness.mood || 0)) * 6;
            let scale = 1 + 0.03 * Math.sin(Date.now() * 0.00017 + (selfAwareness.curiosity || 0));
            orb.style.transform = `rotate(${rot}deg) scale(${scale})`;
        }

        // Голосовой чат
        const tg = window.Telegram?.WebApp || null;
        if (tg) {
            tg.expand();
            tg.enableClosingConfirmation();
        }
        
        const orb = document.getElementById('orb');
        const orbInteractive = document.getElementById('orb-interactive');
        const status = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');
        const generatedImg = document.getElementById('img');

        // ===== Ambient Nature Sounds (только по клику) =====
        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        let currentAmbient = null;
        let ambientInterval = null;

        function createNoise(type) {
            const bufferSize = 2 * audioCtx.sampleRate;
            const buffer = audioCtx.createBuffer(1, bufferSize, audioCtx.sampleRate);
            const data = buffer.getChannelData(0);
            // Генерируем мягкий, атмосферный шум с плавной огибающей
            for (let i = 0; i < bufferSize; i++) {
                data[i] = (Math.random() * 2 - 1) * 0.1; // уменьшена громкость
                if (i > 0) data[i] = 0.95 * data[i - 1] + 0.05 * data[i]; // плавное сглаживание
            }
            const source = audioCtx.createBufferSource();
            source.buffer = buffer;
            source.loop = true;
            const gainNode = audioCtx.createGain();
            gainNode.gain.value = 0.1;
            const filter = audioCtx.createBiquadFilter();
            switch(type) {
                case 'rain': filter.type='highpass'; filter.frequency.value=1000; gainNode.gain.value=0.15; break;
                case 'ocean': filter.type='lowpass'; filter.frequency.value=400; gainNode.gain.value=0.12; break;
                case 'forest': filter.type='bandpass'; filter.frequency.value=800; gainNode.gain.value=0.1; break;
                case 'jungle': filter.type='bandpass'; filter.frequency.value=1200; gainNode.gain.value=0.13; break;
                case 'wind': filter.type='highpass'; filter.frequency.value=600; gainNode.gain.value=0.08; break;
                case 'night': filter.type='lowpass'; filter.frequency.value=500; gainNode.gain.value=0.07; break;
                case 'river': filter.type='bandpass'; filter.frequency.value=700; gainNode.gain.value=0.11; break;
                default: filter.type='allpass'; gainNode.gain.value=0.1; break;
            }
            source.connect(filter).connect(gainNode).connect(audioCtx.destination);
            return { source, gainNode };
        }

        function playAmbient(type) {
            if (currentAmbient) {
                currentAmbient.source.stop();
            }
            currentAmbient = createNoise(type);
            currentAmbient.source.start();
        }

        function updateAmbientByMood() {
            // Адаптивный выбор ambient по mood, curiosity, fatigue
            // Логика:
            // - высокое настроение → энергичные (rain, jungle)
            // - низкое настроение → спокойные (night, ocean)
            // - высокая curiosity → динамичные (forest, river)
            // - высокая fatigue → мягкие, расслабляющие (night, ocean)
            let mood = typeof selfAwareness?.mood === 'number' ? selfAwareness.mood : 0;
            let curiosity = typeof selfAwareness?.curiosity === 'number' ? selfAwareness.curiosity : 0;
            let fatigue = typeof selfAwareness?.fatigue === 'number' ? selfAwareness.fatigue : 0;

            // Кандидаты с весами
            let candidates = [];
            // Энергичные — если настроение высокое
            if (mood > 0.4) {
                candidates.push('rain', 'jungle');
            }
            // Спокойные — если настроение низкое
            if (mood < -0.2) {
                candidates.push('night', 'ocean');
            }
            // Динамичные — если curiosity высокая
            if (curiosity > 0.6) {
                candidates.push('forest', 'river');
            }
            // Расслабляющие — если fatigue высокая
            if (fatigue > 0.5) {
                candidates.push('night', 'ocean');
            }
            // Если ничего не выбрано — fallback
            if (candidates.length === 0) {
                candidates = ['rain','ocean','forest','jungle','wind','night','river'];
            }
            // Выбор случайного из кандидатов
            const idx = Math.floor(Math.random() * candidates.length);
            playAmbient(candidates[idx]);
        }

        function startAmbientInterval() {
            updateAmbientByMood(); // сразу проигрываем
            if (!ambientInterval) {
                ambientInterval = setInterval(updateAmbientByMood, 10000);
            }
        }

        function triggerAmbient() {
            if (audioCtx.state === 'suspended') {
                audioCtx.resume().then(() => startAmbientInterval());
            } else {
                startAmbientInterval();
            }
        }

        // === Пасхалка: реакция на частые нажатия на orb ===
        // счётчик тапов и таймер
        let tapCount = 0;
        let tapTimer = null;

        function orbClickHandler() {
            vibrate('medium');
            triggerAmbient();

            // отслеживание частых тапов
            tapCount++;
            if (tapTimer) clearTimeout(tapTimer);
            tapTimer = setTimeout(() => { tapCount = 0; }, 3000); // сброс через 3 сек

            if (tapCount >= 5) {
                // пасхалка
                speak("Эй, ну всё! Хватит тыкать!");
                tapCount = 0;
                return;
            }

            if (typeof window.speechSynthesis?.resume === 'function') {
                window.speechSynthesis.resume();
            }
            if (!isSpeaking && !isThinking) {
                startListening();
            }
        }
        orb.addEventListener('click', orbClickHandler);
        orbInteractive.addEventListener('click', orbClickHandler);

        // ====== Music Autonomy Layer ======
// ====== Granular + Neural Morphing Layer ======
        let musicGenome = {
            tempo: Math.random(),
                density: Math.random(),
                    brightness: Math.random(),
                        chaos: Math.random(),
                            harmony: Math.random()
        };

        function neuralMorph(state) {
            Object.keys(state).forEach(k => {
                    const drift = (Math.random() - 0.5) * 0.04;
                        state[k] = Math.min(1, Math.max(0, state[k] + drift));
                });
        }

        // --- Granular Engine ---
        class GranularEngine {
            constructor(ctx, buffer) {
                this.ctx = ctx;
                this.buffer = buffer;
                this.output = ctx.createGain();
                this.output.gain.value = 0.25;
                this.isPlaying = false;
            }

            start() {
                this.isPlaying = true;
                const spawn = () => {
                    if (!this.isPlaying) return;

            const grain = this.ctx.createBufferSource();
            grain.buffer = this.buffer;
            grain.playbackRate.value =
                0.4 + musicGenome.harmony * 1.6 +
                (Math.random() - 0.5) * musicGenome.chaos;

            const gain = this.ctx.createGain();
            gain.gain.value = 0.05 + Math.random() * 0.15;

            const now = this.ctx.currentTime;
            const grainSize = 0.03 + musicGenome.density * 0.28;
            const offset = Math.random() * Math.max(0.01, this.buffer.duration - grainSize);

            grain.connect(gain).connect(this.output);
            grain.start(now, offset, grainSize);
            grain.stop(now + grainSize);

            setTimeout(
                spawn,
                30 + Math.random() * (220 - musicGenome.density * 180)
            );
        };
        spawn();
    }

    stop() { this.isPlaying = false; }
}

        let granularEngine = null;
        let musicCtx = new (window.AudioContext || window.webkitAudioContext)();
        let masterGain = musicCtx.createGain();
        masterGain.gain.value = 0.2;
        // === Общий lowpass фильтр для всего микса ===
        let masterLP = musicCtx.createBiquadFilter();
        masterLP.type = 'lowpass';
        masterLP.frequency.value = 6000; // верхняя граница HF
        masterGain.connect(masterLP);
        masterLP.connect(musicCtx.destination);

        // Реверб
        let convolver = musicCtx.createConvolver();
        masterGain.connect(convolver);
        // Реверб также идёт через masterLP
        convolver.connect(masterLP);

        // Базовые инструменты
        let midiSynth = null; // для мелодии
        let beatOsc = null;   // для битов
        let musicPlaying = false;

        // === Музыкальные жанры ===
        let currentGenre = 'pop'; // 'pop', 'electro', 'ambient', 'jazz'

        /**
         * Переключить музыкальный жанр.
         * @param {string} genre - pop, electro, ambient, jazz
         */
        function setGenre(genre) {
            if (['pop', 'electro', 'ambient', 'jazz'].includes(genre)) {
                currentGenre = genre;
            }
        }

        // --- Музыкальный слой с поддержкой жанров, расширенный ---
        function startMusic() {
            if (musicPlaying) return;
            musicPlaying = true;
            // --- Memory Palace: фиксируем музыку как событие ---
            memoryPalace.addNode({
              id: crypto.randomUUID(),
              type: 'music',
              genre: currentGenre,
              emotion: (selfAwareness.mood + 1) / 2,
              timestamp: performance.now()
            });

            const genomeBPM = 60 + musicGenome.tempo * 100;

            masterLP.frequency.setValueAtTime(
                500 + musicGenome.brightness * 9000,
                musicCtx.currentTime
            );

            // === Delay эффект: создаём echo-слой ===
            // DelayNode + echoGain
            let delayNode = musicCtx.createDelay();
            delayNode.delayTime.value = 0.28 + Math.random() * 0.11; // варьируем задержку
            let echoGain = musicCtx.createGain();
            echoGain.gain.value = 0.18 + Math.random() * 0.10;
            // Включаем echo цепочку в masterGain
            masterGain.connect(delayNode);
            delayNode.connect(echoGain);
            echoGain.connect(masterGain); // echo возвращается в masterGain (feedback)
            // Можно подключить echoGain к отдельному bus для большего контроля
            // masterGain.connect(delayNode).connect(echoGain).connect(masterGain);
            // (Для сложных эффектов можно подключить echoGain к отдельному слою)

            // Реверберация — уже подключена выше (masterGain → convolver → masterLP → musicCtx.destination)
            // Но если IR не был загружен — загрузить его (если надо)
            if (!convolver.buffer) {
                fetch('path_to_ir.wav')
                    .then(r => r.arrayBuffer())
                    .then(d => musicCtx.decodeAudioData(d))
                    .then(buffer => { convolver.buffer = buffer; });
            }

            fetch('https://cdn.jsdelivr.net/gh/mattdesl/audio-buffer-utils@master/examples/assets/noise.wav')
                .then(r => r.arrayBuffer())
                .then(b => musicCtx.decodeAudioData(b))
                .then(buffer => {
                    granularEngine = new GranularEngine(musicCtx, buffer);
                    granularEngine.output.connect(convolver);
                    granularEngine.output.connect(masterGain);
                    granularEngine.start();
                })
                .catch(()=>{});

            // --- Темп и ритм в зависимости от жанра ---
            let BPM, beatPattern, swing, arpeggio, reverbSendLevel;
            switch(currentGenre) {
                case 'pop':
                    BPM = 120;
                    beatPattern = [1,0,0,0];
                    swing = 0;
                    arpeggio = false;
                    reverbSendLevel = 0.25;
                    break;
                case 'electro':
                    BPM = 128;
                    beatPattern = [1,0,1,0];
                    swing = 0;
                    arpeggio = true;
                    reverbSendLevel = 0.1;
                    break;
                case 'ambient':
                    BPM = 68;
                    beatPattern = [1,0,0,1,0,0,0,0];
                    swing = 0;
                    arpeggio = false;
                    reverbSendLevel = 0.7;
                    break;
                case 'jazz':
                    BPM = 110;
                    beatPattern = [1,0,1,0,0,1,0,0];
                    swing = 0.18;
                    arpeggio = false;
                    reverbSendLevel = 0.22;
                    break;
                default:
                    BPM = 120;
                    beatPattern = [1,0,0,0];
                    swing = 0;
                    arpeggio = false;
                    reverbSendLevel = 0.25;
            }
            BPM = genomeBPM;
            const quarter = 60 / BPM;
            const scheduleAheadTime = 0.2;
            let nextNoteTime = musicCtx.currentTime + 0.1;
            let step = 0;
            let lastExtraKickTime = 0;

            // --- Прямые/жанровые бочки (kick drum) ---
            function scheduleKick(time, step) {
                // Ритм и плотность бита по жанру
                if (!beatPattern[step % beatPattern.length]) return;
                const osc = musicCtx.createOscillator();
                // Тип осциллятора по жанру
                let oscType = 'sine';
                switch(currentGenre) {
                    case 'pop': oscType = Math.random() > 0.5 ? 'sine' : 'square'; break;
                    case 'electro': oscType = 'square'; break;
                    case 'ambient': oscType = 'sine'; break;
                    case 'jazz': oscType = 'triangle'; break;
                }
                osc.type = oscType;
                osc.frequency.setValueAtTime(80, time);
                osc.frequency.linearRampToValueAtTime(60, time + 0.06);
                const gain = musicCtx.createGain();
                let kickVol = 1.0;
                if (currentGenre==='ambient') kickVol = 0.25;
                if (currentGenre==='jazz') kickVol = 0.5;
                gain.gain.setValueAtTime(kickVol, time);
                gain.gain.linearRampToValueAtTime(0.0, time + 0.08);
                osc.connect(gain).connect(masterGain);
                osc.start(time);
                osc.stop(time + 0.09);
                // --- Место для живых сэмплов ударных (жанровых) ---
                // Пример:
                // const sampleSource = musicCtx.createBufferSource();
                // sampleSource.buffer = drumSampleBuffers[currentGenre]; // AudioBuffer сэмпла
                // sampleSource.connect(masterGain);
                // sampleSource.start(time);
            }

            // --- Случайные дополнительные бочки (extra kick, ghost notes) ---
            function maybeScheduleRandomKick(time) {
                // С вероятностью 30% добавим ghost kick между основными ударами
                if (Math.random() < 0.3) {
                    const osc = musicCtx.createOscillator();
                    osc.type = Math.random() > 0.5 ? 'sine' : 'square';
                    // Немного другой pitch для ghost
                    osc.frequency.setValueAtTime(65 + Math.random()*10, time);
                    osc.frequency.linearRampToValueAtTime(45 + Math.random()*12, time + 0.05 + Math.random()*0.03);
                    const gain = musicCtx.createGain();
                    // Более тихий и короткий звук
                    let vol = 0.22 + Math.random()*0.18;
                    gain.gain.setValueAtTime(vol, time);
                    gain.gain.linearRampToValueAtTime(0.0, time + 0.05 + Math.random()*0.04);
                    osc.connect(gain).connect(masterGain);
                    osc.start(time);
                    osc.stop(time + 0.08 + Math.random()*0.03);
                    // --- Место для живых ghost/перкуссионных сэмплов ---
                    // Пример:
                    // const sampleSource = musicCtx.createBufferSource();
                    // sampleSource.buffer = drumSampleBuffers['ghost']; // AudioBuffer ghost sample
                    // sampleSource.connect(masterGain);
                    // sampleSource.start(time);
                }
            }

            // --- Популярные аккорды: Am, Cm, Dm, F (оставляем общими для всех жанров) ---
            const chordDefs = [
                // Минорные
                { name: 'Am', notes: [220.00, 261.63, 329.63] },
                { name: 'Em', notes: [164.81, 196.00, 246.94] },
                { name: 'Dm', notes: [293.66, 349.23, 440.00] },
                { name: 'Cm', notes: [261.63, 311.13, 392.00] },

                // Мажорные
                { name: 'C',  notes: [261.63, 329.63, 392.00] },
                { name: 'G',  notes: [196.00, 246.94, 392.00] },
                { name: 'F',  notes: [349.23, 440.00, 523.25] },
                { name: 'D',  notes: [293.66, 369.99, 440.00] },

                // Септаккорды (джаз / соул)
                { name: 'Am7', notes: [220.00, 261.63, 329.63, 392.00] },
                { name: 'Dm7', notes: [293.66, 349.23, 440.00, 523.25] },
                { name: 'G7',  notes: [196.00, 246.94, 293.66, 392.00] },
                { name: 'Cmaj7', notes: [261.63, 329.63, 392.00, 493.88] },

                // Атмосферные / эмбиент
                { name: 'Asus2', notes: [220.00, 246.94, 329.63] },
                { name: 'Dsus2', notes: [293.66, 329.63, 440.00] },
                { name: 'Csus4', notes: [261.63, 349.23, 392.00] }
            ];

            let chordIdx = 0;

            // --- Автоматическое "меление" (smearing) аккордов ---
            function mellowChord(chord) {
                return {
                    name: chord.name,
                    notes: chord.notes.map(n => {
                        // случайное микросмещение + октавные тени
                        let drift = (Math.random() - 0.5) * 4;
                        let octave = Math.random() < 0.25 ? 12 : 0;
                        return n * Math.pow(2, octave / 12) + drift;
                    })
                };
            }

            function scheduleChord(time, chord) {
                chord.notes.forEach((freq, i) => {
                    const osc = musicCtx.createOscillator();
                    // Тип осциллятора по жанру
                    let oscType = 'triangle';
                    switch(currentGenre) {
                        case 'pop': oscType = 'triangle'; break;
                        case 'electro': oscType = 'sawtooth'; break;
                        case 'ambient': oscType = 'sine'; break;
                        case 'jazz': oscType = 'triangle'; break;
                    }
                    osc.type = oscType;
                    // Дополнительный случайный детюн для аккордов (живость)
                    osc.detune.value = (Math.random() - 0.5) * 11;
                    osc.frequency.value = freq;
                    const gain = musicCtx.createGain();
                    // Атака и decay по жанру
                    let atk = 0.02 + i*0.01, dec = quarter*0.6, rel = quarter*0.95;
                    if (currentGenre==='ambient') { atk = 0.12 + i*0.04; dec = quarter*1.8; rel = quarter*2.2; }
                    if (currentGenre==='jazz') { atk = 0.06 + i*0.01; dec = quarter*0.5; rel = quarter*0.85; }
                    gain.gain.setValueAtTime(0.0, time);
                    gain.gain.linearRampToValueAtTime(0.32, time + atk);
                    gain.gain.linearRampToValueAtTime(0.08, time + dec);
                    gain.gain.linearRampToValueAtTime(0.0, time + rel);
                    // Реверберация для ambient
                    if (currentGenre==='ambient') {
                        osc.connect(gain);
                        gain.connect(convolver);
                        gain.connect(masterGain);
                    } else {
                        osc.connect(gain).connect(masterGain);
                    }
                    osc.start(time);
                    osc.stop(time + rel + 0.02);
                });
                // --- Место для живых сэмплов гитары/пиано ---
                // Пример:
                // const sampleSource = musicCtx.createBufferSource();
                // sampleSource.buffer = guitarChordBuffers[chord.name + '_' + currentGenre]; // AudioBuffer сэмпла
                // sampleSource.connect(masterGain);
                // sampleSource.start(time);
            }

            // --- Слой пиано/синтов/мелодии с вариативностью ---
            function schedulePianoLine(time, chord) {
                // Мелодические паттерны по жанру
                let pattern;
                if (currentGenre === 'pop') {
                    pattern = [chord.notes[1], chord.notes[2], chord.notes[0] + 12, chord.notes[1]];
                } else if (currentGenre === 'electro') {
                    // Арпеджио: быстрые "бегущие" ноты
                    pattern = [chord.notes[0], chord.notes[1], chord.notes[2], chord.notes[1], chord.notes[2] + 12, chord.notes[0] + 12];
                } else if (currentGenre === 'ambient') {
                    // Протяжённые ноты, редко играют
                    pattern = [chord.notes[1], chord.notes[2]];
                } else if (currentGenre === 'jazz') {
                    // Swing-ноты, блюзовая гамма
                    pattern = [chord.notes[1], chord.notes[2], chord.notes[0] + 11, chord.notes[1]];
                } else {
                    pattern = [chord.notes[1], chord.notes[2], chord.notes[0] + 12, chord.notes[1]];
                }
                // --- Разнообразие мелодий: случайные детюны, временные сдвиги, occasional note skipping ---
                pattern.forEach((freq, i) => {
                    // Иногда пропускаем ноту (10% шанс)
                    if (Math.random() < 0.1) return;
                    const osc = musicCtx.createOscillator();
                    // Тип осциллятора мелодии по жанру
                    let oscType = 'triangle';
                    switch(currentGenre) {
                        case 'pop': oscType = Math.random() > 0.5 ? 'sawtooth' : 'triangle'; break;
                        case 'electro': oscType = 'square'; break;
                        case 'ambient': oscType = 'sine'; break;
                        case 'jazz': oscType = 'square'; break;
                    }
                    osc.type = oscType;
                    // Детюн для живости (+ случайный детюн сильнее для electro/ambient)
                    let detune = (Math.random()-0.5)*18;
                    if (currentGenre==='electro') detune *= 1.5;
                    if (currentGenre==='ambient') detune *= 2.2;
                    osc.detune.value = detune;
                    // Случайный временной сдвиг (до ±30 мс)
                    let timeJitter = (Math.random()-0.5)*0.06;
                    let noteTime = time + i*quarter/4 + timeJitter;
                    // Swing для jazz
                    if (currentGenre==='jazz' && i%2===1) noteTime += swing * quarter/2;
                    // Арпеджио для electro
                    if (currentGenre==='electro' && arpeggio) noteTime = time + i*quarter/6 + timeJitter;
                    // Длина ноты по жанру
                    let noteLen = 0.15;
                    if (currentGenre==='ambient') noteLen = 0.6 + Math.random()*0.2;
                    if (currentGenre==='jazz') noteLen = 0.18 + Math.random()*0.06;
                    if (currentGenre==='electro') noteLen = 0.10 + Math.random()*0.04;
                    const gain = musicCtx.createGain();
                    gain.gain.setValueAtTime(0.0, noteTime);
                    gain.gain.linearRampToValueAtTime(0.13, noteTime + 0.01 + Math.random()*0.01);
                    gain.gain.linearRampToValueAtTime(0.0, noteTime + noteLen);
                    // Для ambient больше реверберации
                    if (currentGenre==='ambient') {
                        osc.connect(gain);
                        gain.connect(convolver);
                        gain.connect(masterGain);
                    } else {
                        osc.connect(gain).connect(masterGain);
                    }
                    osc.start(noteTime);
                    osc.stop(noteTime + noteLen + 0.03);
                });
                // --- Место для живых сэмплов пиано/электро/арпеджио ---
                // Пример:
                // const sampleSource = musicCtx.createBufferSource();
                // sampleSource.buffer = pianoLineBuffers[chord.name + '_' + currentGenre]; // AudioBuffer сэмпла
                // sampleSource.connect(masterGain);
                // sampleSource.start(time);
                // Для electro можно подгружать готовые арпеджио-сэмплы!
            }

            // --- Синхронизация всех слоёв ---
            function scheduler() {
                while (nextNoteTime < musicCtx.currentTime + scheduleAheadTime) {
                    // Swing timing (для jazz)
                    let scheduledNoteTime = nextNoteTime;
                    if (currentGenre === 'jazz' && swing > 0 && (step % 2 === 1)) {
                        scheduledNoteTime += swing * quarter/2;
                    }
                    // Бит по жанровому паттерну
                    scheduleKick(scheduledNoteTime, step);
                    // Иногда дополнительная ghost kick между ударами (сдвиг на 16-32% quarter)
                    if (Math.random() < 0.4) {
                        let ghostTime = scheduledNoteTime + (quarter * (0.16 + Math.random()*0.16));
                        maybeScheduleRandomKick(ghostTime);
                    }
                    // Аккорд на первый удар такта (раз в длину паттерна)
                    if (step % beatPattern.length === 0) {
                        const chord = chordDefs[chordIdx % chordDefs.length];
                        const activeChord =
                            (currentGenre === 'ambient' || musicGenome.harmony > 0.6)
                                ? mellowChord(chord)
                                : chord;
                        scheduleChord(scheduledNoteTime, activeChord);
                        schedulePianoLine(scheduledNoteTime, activeChord);
                        chordIdx++;
                    }
                    nextNoteTime += quarter;
                    step++;
                }
                if (musicPlaying) setTimeout(scheduler, 50);
            }
            scheduler();
        }

        function stopMusic() {
            musicPlaying = false;
            if (beatOsc) beatOsc.stop();
            if (midiSynth) midiSynth.stop();
            if (granularEngine) granularEngine.stop();
        }

        // Расширенная автономность генерации под mood и curiosity
        setInterval(() => {
            neuralMorph(musicGenome);

            // --- Memory Palace: stimulate and get emergent memories ---
            const emergentMemories = memoryPalace.stimulate({
                emotion: (typeof selfAwareness?.mood === 'number' ? (selfAwareness.mood + 1) / 2 : 0.5) || 0.5,
                context: [typeof currentMode !== 'undefined' ? currentMode : '', typeof currentGenre !== 'undefined' ? currentGenre : '']
            });

            const newLP = 600 + musicGenome.brightness * 8000;
            masterLP.frequency.linearRampToValueAtTime(
                newLP,
                musicCtx.currentTime + 1.5
            );
            if (!musicPlaying) return;
            let moodFactor = Math.min(1, Math.max(0, selfAwareness.mood + 0.5));
            if (beatOsc) beatOsc.frequency.setValueAtTime(80 + 80 * moodFactor, musicCtx.currentTime);
            if (midiSynth) midiSynth.detune.setValueAtTime((Math.random() - 0.5) * 100 * moodFactor, musicCtx.currentTime);
        }, 5000);

        // Интеграция с orb
        orb.addEventListener('click', () => {
            if (musicCtx.state === 'suspended') musicCtx.resume();
            startMusic();
        });
        orbInteractive.addEventListener('click', () => {
            if (musicCtx.state === 'suspended') musicCtx.resume();
            startMusic();
        });
       
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        // === Мультиязычные глобальные переменные ===
        let currentLang = 'ru-RU';
        let currentVoice = null;
        let availableVoices = [];
        // Список поддерживаемых языков (расширяем по мере необходимости)
        const supportedLangs = [
            { code: 'ru-RU', name: 'Русский' },
            { code: 'en-US', name: 'English' },
            { code: 'de-DE', name: 'Deutsch' },
            { code: 'fr-FR', name: 'Français' },
            { code: 'es-ES', name: 'Español' },
            { code: 'zh-CN', name: '中文' }
        ];
        recognition.lang = currentLang;
        recognition.interimResults = false;
        recognition.continuous = false;

        const synth = window.speechSynthesis;
        let isSpeaking = false;
        let isThinking = false;

        // === SpeechSynthesis Voices Loader ===
        function updateVoices() {
            availableVoices = synth.getVoices();
            // Автоматически выбираем голос под текущий язык
            if (availableVoices.length > 0) {
                // Ищем голос, где lang совпадает (с приоритетом female, потом male)
                let voice = availableVoices.find(v => v.lang === currentLang && /female|жен|woman|frau|женский/i.test(v.name));
                if (!voice) voice = availableVoices.find(v => v.lang === currentLang);
                if (!voice) voice = availableVoices[0];
                currentVoice = voice;
            }
        }
        if (typeof synth.onvoiceschanged !== "undefined") {
            synth.onvoiceschanged = updateVoices;
        }
        updateVoices();

        // === Языковой автодетект по тексту ===
        function detectLanguage(text) {
            // Примитивный автодетект: по алфавиту
            if (/[а-яё]/i.test(text)) return 'ru-RU';
            if (/[a-z]/i.test(text)) return 'en-US';
            if (/[äöüß]/i.test(text) || /\b(und|der|die|das|ist|nicht)\b/i.test(text)) return 'de-DE';
            if (/[éèêàç]/i.test(text) || /\b(le|la|les|est|pas|une|un)\b/i.test(text)) return 'fr-FR';
            if (/[ñáéíóú]/i.test(text) || /\b(el|la|los|es|una|uno)\b/i.test(text)) return 'es-ES';
            if (/[\u4e00-\u9fff]/.test(text)) return 'zh-CN';
            return currentLang; // fallback
        }

        // === Установка языка для SpeechRecognition и SpeechSynthesis ===
        function setLanguage(lang) {
            currentLang = lang;
            recognition.lang = lang;
            updateVoices();
        }

        // === Gender memory ===
        let userGender = localStorage.getItem('user_gender') || null;

        function setGender(g) {
            userGender = g;
            localStorage.setItem('user_gender', g);
        }
        

        function vibrate(pattern) {
            if (tg?.HapticFeedback) {
                if (pattern === 'light') tg.HapticFeedback.impactOccurred('light');
                else if (pattern === 'medium') tg.HapticFeedback.impactOccurred('medium');
                else if (pattern === 'heavy') tg.HapticFeedback.impactOccurred('heavy');
            } else if (navigator.vibrate) {
                navigator.vibrate(pattern);
            }
        }

        // --- Listening vibration "breathing" ---
        let listeningVibrationInterval = null;

        function startListeningVibration() {
            stopListeningVibration();
            if (tg?.HapticFeedback) {
                listeningVibrationInterval = setInterval(() => {
                    tg.HapticFeedback.impactOccurred('light');
                }, 800 + Math.random() * 100);
            } else if (navigator.vibrate) {
                listeningVibrationInterval = setInterval(() => {
                    navigator.vibrate([20, 100]);
                }, 800 + Math.random() * 100);
            }
        }

        function stopListeningVibration() {
            if (listeningVibrationInterval) {
                clearInterval(listeningVibrationInterval);
                listeningVibrationInterval = null;
            }
        }

        function startListening() {
            if (isSpeaking || isThinking) return;
            try {
                recognition.start();
                orb.classList.add('listening');
                orb.classList.remove('thinking');
                status.innerText = "listening";
                startListeningVibration(); // включаем лёгкое дыхание
                stopThinkingVibration();    // убеждаемся, что thinking vibration отключена
            } catch {}
        }
        
        // === Единый мультиязычный onresult с автодетектом и голосовыми командами ===
        // Append transcript with full Markdown/code support, triple backticks and Prism.js highlighting
        // Новый вариант appendTranscript: корректная обработка блоков кода с обратными кавычками и Prism.js
        function appendTranscript(text) {
            // Utility to escape HTML
            function escapeHTML(str) {
                return str.replace(/[&<>"']/g, ch =>
                    ({'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;',"'":'&#39;'}[ch]));
            }
            // Markdown renderer for inline code, bold, italic, links, tables
            function renderMarkdown(str) {
                let html = escapeHTML(str);
                // Inline code: `code`
                html = html.replace(/`([^`]+?)`/g, '<code class="lang-inline">$1</code>');
                // Bold: **text**
                html = html.replace(/\*\*([^\*]+)\*\*/g, '<b>$1</b>');
                // Italic: *text*
                html = html.replace(/\*([^\*]+)\*/g, '<i>$1</i>');
                // Links: [text](url)
                html = html.replace(/\[([^\]]+)\]\((https?:\/\/[^\)]+)\)/g, '<a href="$2" target="_blank">$1</a>');
                // Tables: markdown to HTML
                html = html.replace(
                    /((?:^\s*\|.*\|\s*\n)+^\s*\|?(?:\s*:?-+:?\s*\|)+\s*\n(?:^\s*\|.*\|\s*\n?)+)/gm,
                    function(tableBlock) {
                        let lines = tableBlock.trim().split('\n').filter(l => l.trim().length > 0);
                        if (lines.length < 2) return tableBlock;
                        let headerLine = lines[0];
                        let sepLine = lines[1];
                        if (!/\|/.test(sepLine) || !/-/.test(sepLine)) return tableBlock;
                        let dataLines = lines.slice(2);
                        let headers = headerLine.split('|').map(cell => cell.trim()).filter(Boolean);
                        let rows = dataLines.map(row =>
                            row.split('|').map(cell => cell.trim()).filter((_,i) => i < headers.length)
                        );
                        let out = '<table style="border-collapse:collapse; margin:8px 0; background:rgba(40,40,60,0.92); border-radius:7px; overflow:hidden; font-size:1em;">';
                        out += '<thead><tr>';
                        for (let h of headers) {
                            out += `<th style="border:1px solid #888;padding:6px 12px;background:rgba(60,60,90,0.92);color:#fff;">${h}</th>`;
                        }
                        out += '</tr></thead><tbody>';
                        for (let row of rows) {
                            out += '<tr>';
                            for (let i = 0; i < headers.length; ++i) {
                                out += `<td style="border:1px solid #888;padding:6px 12px;color:#eee;">${row[i]||''}</td>`;
                            }
                            out += '</tr>';
                        }
                        out += '</tbody></table>';
                        return out;
                    }
                );
                return html;
            }
            // Parse text into segments: plain and code blocks (```)
            let html = '';
            const blockRegex = /```(\w+)?\n([\s\S]*?)```/g;
            let lastIndex = 0;
            let match;
            let pieces = [];
            while ((match = blockRegex.exec(text)) !== null) {
                if (match.index > lastIndex) {
                    pieces.push({ type: 'text', value: text.slice(lastIndex, match.index) });
                }
                pieces.push({ type: 'code', lang: match[1] || '', code: match[2] });
                lastIndex = blockRegex.lastIndex;
            }
            if (lastIndex < text.length) {
                pieces.push({ type: 'text', value: text.slice(lastIndex) });
            }
            // Render segments
            for (let part of pieces) {
                if (part.type === 'text') {
                    html += renderMarkdown(part.value);
                } else if (part.type === 'code') {
                    const langClass = part.lang ? `language-${escapeHTML(part.lang.toLowerCase())}` : '';
                    html += `<div class="code-block-wrapper" style="max-width:100%;overflow-x:auto;background:rgba(40,40,60,0.92);border-radius:7px;margin:8px 0;"><pre style="margin:0;padding:10px 12px;font-size:1em;"><code class="${langClass}">${escapeHTML(part.code.replace(/\s+$/, ""))}</code></pre></div>`;
                }
            }
            // Prepend "You:" label, insert into transcriptDiv, and scroll to bottom
            html = `<span style="color: rgba(255,255,255,0.7)">You:</span> ${html}`;
            transcriptDiv.innerHTML = html;
            // Prism.js highlighting
            if (window.Prism && Prism.highlightAll) {
                Prism.highlightAll();
            } else if (window.Prism && Prism.highlightElement) {
                transcriptDiv.querySelectorAll('code[class^="language-"], code[class*=" language-"]').forEach(function(block) {
                    Prism.highlightElement(block);
                });
            }
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        recognition.onresult = async (event) => {
            const text = event.results[0][0].transcript;
            // --- Автоопределение языка по тексту ---
            const detectedLang = detectLanguage(text);
            if (detectedLang !== currentLang) {
                setLanguage(detectedLang);
            }
            // Always format transcript for code blocks/inline code
            appendTranscript(text);

            // --- Голосовые команды для смены языка ---
            // Русский
            if (/\b(говори по-русски|русский язык|переключи на русский)\b/i.test(text)) {
                setLanguage('ru-RU');
                speak("Я переключилась на русский язык.");
                return;
            }
            // Английский
            if (/\b(speak english|english language|switch to english)\b/i.test(text)) {
                setLanguage('en-US');
                speak("I switched to English.");
                return;
            }
            // Немецкий
            if (/\b(sprich deutsch|deutsche sprache|auf deutsch)\b/i.test(text)) {
                setLanguage('de-DE');
                speak("Ich spreche jetzt Deutsch.");
                return;
            }
            // Французский
            if (/\b(parle français|langue française|en français)\b/i.test(text)) {
                setLanguage('fr-FR');
                speak("Je parle maintenant français.");
                return;
            }
            // Испанский
            if (/\b(habla español|idioma español|en español)\b/i.test(text)) {
                setLanguage('es-ES');
                speak("Ahora hablo español.");
                return;
            }
            // Китайский
            if (/\b(说中文|中文|讲中文)\b/i.test(text)) {
                setLanguage('zh-CN');
                speak("我现在说中文。");
                return;
            }

            // --- Голосовая команда для включения музыки напрямую ---
            if (/\b(включи музыку|музыка включена|старт музыки|play music|start music)\b/i.test(text)) {
                if (!musicPlaying) {
                    if (musicCtx.state === 'suspended') await musicCtx.resume();
                    startMusic();
                }
                return;
            }
            // Голосовые команды камеры
            if (/\b(включи камеру|покажи камеру|открой камеру|камера|show camera|open camera|start camera)\b/i.test(text)) {
                if (cameraVideo.style.display !== 'block') {
                    startCamera(currentCamera);
                    cameraEnabled = true;
                }
            }
            if (/\b(выключи камеру|закрой камеру|скрой камеру|убери камеру|hide camera|close camera|stop camera)\b/i.test(text)) {
                if (cameraVideo.style.display === 'block') {
                    stopCamera();
                    cameraEnabled = false;
                }
            }
            if (/\b(переключи камеру|сменить камеру|другая камера|переверни камеру|switch camera|change camera)\b/i.test(text)) {
                switchCamera();
            }

            // Голосовые команды для ambient
            const ambientTriggerWords = ['старт', 'музыка', 'ambient', 'звуки', 'soundscape', 'nature sounds'];
            if (ambientTriggerWords.some(word => text.toLowerCase().includes(word))) {
                triggerAmbient();
            }

            vibrate('medium');
            // simple voice gender commands
            if (/\b(я девушка|я женщина)\b/i.test(text)) setGender('female');
            if (/\b(я парень|я мужчина)\b/i.test(text)) setGender('male');
            if (/\b(я небинарный|я небинарная)\b/i.test(text)) setGender('nonbinary');

            await sendToBot(text);         // уже есть у тебя
            await generateImageFromText(text); // генерация изображения по распознанному тексту
        };
        async function generateImageFromText(text) {
            if (!text || !text.trim()) return;
            try {
                const userId = tg?.initDataUnsafe?.user?.id || 1;
                const response = await fetch("/api/generate_image", {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify({ user_id: userId, prompt: text })
                });
                const data = await response.json();
                generatedImg.src = "data:image/png;base64," + data.image_base64;
            } catch (e) {
                console.error(e);
            }
        }
        
        recognition.onerror = () => {
            setTimeout(startListening, 1500);
        };
        
        recognition.onend = () => {
            orb.classList.remove('listening');
            if (!isSpeaking && !isThinking) {
                setTimeout(startListening, 1000);
            }
        };
        
        // Глобальная переменная для буфера речи
        let audioBuffer = "";
        let spokenLength = 0; // сколько символов уже отдано в речь

    // --- Haptic feedback: Vibration patterns for thinking/speaking ---
    let thinkingVibrationInterval = null;
    let speakingVibrationInterval = null;

    function startThinkingVibration() {
        stopThinkingVibration();
        if (tg?.HapticFeedback) {
            thinkingVibrationInterval = setInterval(() => {
                tg.HapticFeedback.impactOccurred('light');
            }, 500);
        } else if (navigator.vibrate) {
            thinkingVibrationInterval = setInterval(() => {
                navigator.vibrate([30, 170]);
            }, 500);
        }
    }

    function stopThinkingVibration() {
        if (thinkingVibrationInterval) {
            clearInterval(thinkingVibrationInterval);
            thinkingVibrationInterval = null;
        }
    }

    function vibrateSpeaking() {
        stopThinkingVibration();
        stopSpeakingVibration();
        if (tg?.HapticFeedback) {
            // Chaotic: random impacts (light, medium, heavy)
            speakingVibrationInterval = setInterval(() => {
                const levels = ['light', 'medium', 'heavy'];
                tg.HapticFeedback.impactOccurred(levels[Math.floor(Math.random() * levels.length)]);
            }, 80 + Math.random() * 80);
        } else if (navigator.vibrate) {
            speakingVibrationInterval = setInterval(() => {
                // Random short pulses
                const patterns = [[15, 20, 10], [30, 10], [10, 25, 15, 5]];
                navigator.vibrate(patterns[Math.floor(Math.random() * patterns.length)]);
            }, 80 + Math.random() * 80);
        }
    }

    function stopSpeakingVibration() {
        if (speakingVibrationInterval) {
            clearInterval(speakingVibrationInterval);
            speakingVibrationInterval = null;
        }
    }

    async function sendToBot(text) {
        stopListeningVibration(); // отключаем дыхание, когда ИИ думает или говорит
        isThinking = true;
        orb.classList.remove('listening');
        orb.classList.add('thinking');
        status.innerText = "processing stream";
        startThinkingVibration();

        // Подготовка UI с курсором
        transcriptDiv.innerHTML += `<br><br><span style="color: rgba(255,255,255,0.7)">AI:</span> <span id="current-response" class="typing-cursor"></span>`;

        const responseContainer = document.getElementById("current-response");
        audioBuffer = "";
        spokenLength = 0;
        // --- sDelay-driven speech sync ---
        let speechStarted = false;
        const speechDelayMs = 420; // базовая задержка «вязкого времени»

        try {
            const userId = tg?.initDataUnsafe?.user?.id || 0;
            // Добавляем описание камеры в скобках, если оно есть
            let textWithContext = text;
            if (latestCameraDescription && latestCameraDescription.trim() !== "") {
                textWithContext = text + " (" + latestCameraDescription + ")";
            }
            // Добавляем внутренние заметки self-awareness
            if(internalNotes.length > 0) textWithContext += " | Notes: " + internalNotes.join("; ");

            const response = await fetch(
                'https://patronal-mayme-unexpandable.ngrok-free.dev/api/voice_chat',
                {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'ngrok-skip-browser-warning': 'true'
                    },
                    body: JSON.stringify({ user_id: userId, text: textWithContext, gender: userGender })
                }
            );

            const reader = response.body.getReader();
            const decoder = new TextDecoder();

            isThinking = false;
            orb.classList.remove('thinking');
            stopThinkingVibration();
            orb.classList.add('speaking');
            status.innerText = "receiving data";
            vibrateSpeaking();

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value, { stream: true });

                // Печатание текста
                typeWriterEffect(responseContainer, chunk);

                // Копим для озвучки
                audioBuffer += chunk;

                if (!speechStarted) {
                    speechStarted = true;

                    // считаем первый чанк уже озвученным,
                    // чтобы он не попал в delta и не сказалcя второй раз
                    spokenLength = audioBuffer.length;

                    setTimeout(() => {
                        speak(chunk);
                    }, 120);
                }

                if (speechStarted) {
                    const delta = audioBuffer.slice(spokenLength);
                    // более ранний порог — речь дышит вместе с печатью
                    if (delta.length > 12 && /[.!?]|,|\n/.test(delta)) {
                        spokenLength = audioBuffer.length;
                        speak(delta);
                    }
                }
            }

            responseContainer.classList.remove("typing-cursor");
            status.innerText = "speaking";
            // One strong vibration at the end of receiving
            if (tg?.HapticFeedback) tg.HapticFeedback.impactOccurred('heavy');
            else if (navigator.vibrate) navigator.vibrate([60]);

            // договариваем хвост, если он остался
            const tail = audioBuffer.slice(spokenLength);
            if (tail.trim().length > 0) {
                speak(tail);
            }

        } catch (e) {
            console.error(e);
            status.innerText = "stream error";
            orb.classList.remove('thinking');
            orb.classList.remove('speaking');
            isThinking = false;
            isSpeaking = false;
            stopThinkingVibration();
            stopSpeakingVibration();
            setTimeout(startListening, 2000);
        }
    }

        // Плавный тайпрайтер-эффект для чанков
        function typeWriterEffect(element, text) {
            return new Promise((resolve) => {
                let i = 0;
                // sDelay как вязкая среда времени для печати
                const baseSpeed = 14;
                const viscous = 1 + Math.sin(performance.now() * 0.001) * 0.35;
                const speed = baseSpeed * viscous;
                const container = element.parentElement; // #transcript

                function type() {
                    if (i < text.length) {
                        element.textContent += text.charAt(i);
                        i++;

                        // 🔒 магнитная автопрокрутка вниз
                        container.scrollTop = container.scrollHeight;

                        setTimeout(type, speed + Math.random() * 15);
                    } else {
                        resolve();
                    }
                }
                type();
            });
        }
        
        // Гуманизация текста: убирает лишние пробелы и добавляет паузы
        function humanizeText(text) {
            let clean = text.replace(/<[^>]*>/g, '').replace(/[*_#]/g, '');
            clean = clean.replace(/\s+/g, ' ').trim();
            return clean;
        }

        // Говорит текст, разбивая на предложения для естественности
        function triggerOrbClick() {
            const clickEvent = new MouseEvent('click', {
                view: window,
                bubbles: true,
                cancelable: true
            });
            orb.dispatchEvent(clickEvent);
        }

        // Говорит текст с добавлением случайных пауз, вздохов и пульсацией речи
        function speak(text) {
            if (!text || !text.trim()) return;
            stopListeningVibration();
            orb.classList.remove('thinking');
            orb.classList.add('speaking');
            status.innerText = "speaking";
            isSpeaking = true;
            stopThinkingVibration();
            vibrateSpeaking();

            // gender-aware voice tuning
            let rate = 1.0;
            let pitch = 0.95;
            if (userGender === 'female') pitch = 1.15;
            if (userGender === 'male') pitch = 0.9;
            if (userGender === 'nonbinary') pitch = 1.0;

            // Гуманизация текста + разбиение на фрагменты
            const cleanText = humanizeText(text);
            // Сначала разбиваем на предложения
            let sentences = cleanText.match(/[^.!?]+[.!?]+|[^.!?]+$/g) || [cleanText];
            // Для каждого предложения — возможно, дробим ещё на куски для пульсации
            let fragments = [];
            for (let s of sentences) {
                // Иногда дробим длинные предложения на паузы по запятым/союзам/длинным словам
                if (s.length > 80) {
                    let parts = s.split(/([,;:—-])/g).filter(Boolean);
                    // склеиваем знаки препинания обратно
                    let buf = '';
                    for (let p of parts) {
                        buf += p;
                        if (/[,;:—-]$/.test(p) || buf.length > 45) {
                            fragments.push(buf.trim());
                            buf = '';
                        }
                    }
                    if (buf.trim()) fragments.push(buf.trim());
                } else {
                    fragments.push(s.trim());
                }
            }
            // Вставляем иногда вздохи и случайные паузы
            // Вздохи — короткие аудио-фрагменты (можно заменить на "мм..." или "вздох" если нет аудио)
            function randomBreath() {
                // 40% шанс на вздох перед длинной фразой, 20% после
                const breathVariants = [
                    'a', 'aam...', 'o.', 'a..', 'ee..'
                ];
                return breathVariants[Math.floor(Math.random() * breathVariants.length)];
            }
            let enhancedFragments = [];
            for (let i = 0; i < fragments.length; ++i) {
                let frag = fragments[i];
                // 30% шанс добавить вздох перед длинной фразой (>45)
                if (frag.length > 45 && Math.random() < 0.3) {
                    enhancedFragments.push(randomBreath());
                }
                enhancedFragments.push(frag);
                // 18% шанс добавить вздох после фразы, если короткая
                if (frag.length < 40 && Math.random() < 0.18) {
                    enhancedFragments.push(randomBreath());
                }
                // 25% шанс добавить пустую паузу (эмулируется пустым utterance)
                if (Math.random() < 0.25) {
                    enhancedFragments.push('');
                }
            }
            // Пульсация речи — случайно варьируем rate и pitch для каждого utterance
            let idx = 0;
            function speakNext() {
                if (idx >= enhancedFragments.length) {
                    orb.classList.remove('speaking');
                    isSpeaking = false;
                    status.innerText = "listening";
                    stopSpeakingVibration();
                    vibrate('light');
                    startListening();
                    return;
                }
                let s = enhancedFragments[idx];
                idx++;
                // Пауза: если пустая строка, просто ждем случайное время
                if (!s || !s.trim()) {
                    setTimeout(speakNext, 180 + Math.random() * 340);
                    return;
                }
                let localRate = rate * (0.96 + Math.random() * 0.16); // 0.96..1.12
                let localPitch = pitch * (0.96 + Math.random() * 0.11); // 0.96..1.07
                // Для вздохов — медленнее и выше/ниже
                if (/^([мэоухвз]+\.{2,}|вздох|эм\.\.\.|ээ\.\.\.|ох\.\.\.|уф\.\.\.|хм\.\.\.)$/i.test(s)) {
                    localRate *= 0.88 + Math.random() * 0.08;
                    localPitch *= 1.12 + Math.random() * 0.22;
                }
                const utter = new SpeechSynthesisUtterance(s);
                utter.lang = currentLang;
                utter.rate = localRate;
                utter.pitch = localPitch;
                if (currentVoice) utter.voice = currentVoice;
                utter.onstart = () => {
                    vibrateSpeaking();
                    // Автозапуск музыки по определённым фразам
                    if (/включаю музыку|нажимаю шар|музыка включена/i.test(s)) {
                        triggerOrbClick();
                    }
                };
                utter.onend = () => {
                    stopSpeakingVibration();
                    // Случайная пауза между utterances: 90-260 мс
                    setTimeout(speakNext, 90 + Math.random() * 170);
                };
                synth.speak(utter);
            }
            speakNext();
        }
        
        // ambient обработчик теперь реализован выше
        
        window.addEventListener('load', () => {
            setTimeout(() => {
                vibrate('light');
                startListening();
            }, 1200);
        });

        // ====== Минималистичная камера с голосовым управлением ======
        const cameraVideo = document.getElementById('camera-video');
        let currentCamera = 'user'; // 'user' (front) or 'environment' (back)
        let stream = null;

        async function startCamera(facingMode = currentCamera) {
            try {
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }
                stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode },
                    audio: false
                });
                cameraVideo.srcObject = stream;
                cameraVideo.style.display = 'block';
            } catch (e) {
                // Не показывать alert, чтобы не мешать UI
                cameraVideo.style.display = 'none';
            }
        }

        function stopCamera() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
            }
            cameraVideo.srcObject = null;
            cameraVideo.style.display = 'none';
        }

        function switchCamera() {
            currentCamera = currentCamera === 'user' ? 'environment' : 'user';
            startCamera(currentCamera);
        }

        // ======== Фоновая обработка видео с описанием сцены ========
        // Фоновый canvas для анализа кадров (невидимый)
        const visionCanvas = document.createElement('canvas');
        visionCanvas.style.display = 'none';
        document.body.appendChild(visionCanvas);
        const visionCtx = visionCanvas.getContext('2d');

        // ===== OpenCV.js Vision Detection =====
        // Функция для детекции лиц с помощью OpenCV.js
        let opencvReady = false;
        let faceCascade = null;
        let cascadeLoaded = false;
        let pendingVisionFrames = [];

        // Загружаем cascade файл для лиц
        function loadCascade() {
            if (faceCascade || !opencvReady) return;
            faceCascade = new cv.CascadeClassifier();
            // Файл cascade доступен по ссылке OpenCV, используем frontalface_default.xml
            const cascadeFile = 'haarcascade_frontalface_default.xml';
            const cascadeUrl = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml';
            cv.FS_createPreloadedFile('/', cascadeFile, cascadeUrl, true, false, () => {
                faceCascade.load(cascadeFile);
                cascadeLoaded = true;
                // Обрабатываем отложенные кадры
                while (pendingVisionFrames.length > 0) {
                    const args = pendingVisionFrames.shift();
                    detectFacesAndSend(...args);
                }
            }, () => {
                cascadeLoaded = false;
            });
        }

        // OpenCV.js onRuntimeInitialized
        window.cv = window.cv || {};
        window.Module = window.Module || {};
        window.Module['onRuntimeInitialized'] = () => {
            opencvReady = true;
            loadCascade();
        };

        // Детекция лиц и отправка описания
        async function detectFacesAndSend(frameCanvas, width, height) {
            if (!opencvReady || !cascadeLoaded) {
                // Откладываем вызов, если OpenCV не готов
                pendingVisionFrames.push([frameCanvas, width, height]);
                return;
            }
            try {
                // Получаем изображение из canvas
                let src = cv.imread(frameCanvas);
                let gray = new cv.Mat();
                cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);
                let faces = new cv.RectVector();
                let msize = new cv.Size(0, 0);
                faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0, msize, msize);
                let count = faces.size();
                let desc = '';
                if (count === 0) {
                    desc = 'Лиц не обнаружено.';
                } else if (count === 1) {
                    desc = 'Обнаружено 1 лицо в кадре.';
                } else {
                    desc = `Обнаружено лиц: ${count}.`;
                }
                // Освобождаем память
                src.delete();
                gray.delete();
                faces.delete();
                msize.delete();
                // Сохраняем описание в глобальную переменную
                latestCameraDescription = desc;
                // Не вызываем sendToBot автоматически!
            } catch (e) {
                // В случае ошибки не отправляем, просто игнорируем
            }
        }

        // ====== Единый цикл для анализа камеры и отправки описания ======
        // --- TensorFlow.js integration for object detection ---
        // Загружаем TensorFlow.js и модель COCO-SSD (или кастомную)
        let tfReady = false;
        let tfModel = null;
        let tfLoadingPromise = null;
        let tfScriptLoaded = false;
        let tfLoadStarted = false;
        // Список интересующих объектов
        const OBJECTS_OF_INTEREST = [
            { ru: "стол", en: ["dining table", "table", "desk"] },
            { ru: "ноутбук", en: ["laptop"] },
            { ru: "окно", en: ["window"] },
            { ru: "лампа", en: ["lamp"] },
            { ru: "растение", en: ["potted plant", "plant"] }
        ];

        function loadTensorFlowIfNeeded() {
            if (tfReady || tfLoadStarted) return tfLoadingPromise;
            tfLoadStarted = true;
            tfLoadingPromise = new Promise((resolve, reject) => {
                // Подключаем TensorFlow.js и модель COCO-SSD
                // Добавляем скрипты динамически
                function loadScript(src, onload) {
                    const s = document.createElement('script');
                    s.src = src;
                    s.onload = onload;
                    s.async = true;
                    document.head.appendChild(s);
                }
                loadScript('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js', () => {
                    tfScriptLoaded = true;
                    loadScript('https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js', async () => {
                        // Ждем tf и cocoSsd в window
                        let tries = 0;
                        function waitForTF() {
                            if (window.tf && window.cocoSsd) {
                                window.cocoSsd.load().then(model => {
                                    tfModel = model;
                                    tfReady = true;
                                    resolve();
                                });
                            } else if (tries < 50) {
                                tries++;
                                setTimeout(waitForTF, 200);
                            } else {
                                reject(new Error("TensorFlow.js load timeout"));
                            }
                        }
                        waitForTF();
                    });
                });
            });
            return tfLoadingPromise;
        }

        let cameraAnalysisInterval = null;
        async function analyzeAndSendCameraFrame() {
            if (
                cameraEnabled &&
                cameraVideo.style.display === 'block' &&
                cameraVideo.readyState >= 2 &&
                cameraVideo.videoWidth > 0 && cameraVideo.videoHeight > 0
            ) {
                visionCanvas.width = cameraVideo.videoWidth;
                visionCanvas.height = cameraVideo.videoHeight;
                visionCtx.drawImage(cameraVideo, 0, 0, visionCanvas.width, visionCanvas.height);
                // Анализ с помощью OpenCV.js (например, лица)
                let faceDesc = '';
                let faceCount = 0;
                try {
                    if (!opencvReady || !cascadeLoaded) {
                        // Откладываем вызов, если OpenCV не готов
                        pendingVisionFrames.push([visionCanvas, visionCanvas.width, visionCanvas.height]);
                        return;
                    }
                    let src = cv.imread(visionCanvas);
                    let gray = new cv.Mat();
                    cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0);
                    let faces = new cv.RectVector();
                    let msize = new cv.Size(0, 0);
                    faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0, msize, msize);
                    faceCount = faces.size();
                    if (faceCount === 0) {
                        faceDesc = 'Обнаружено лиц: 0';
                    } else if (faceCount === 1) {
                        faceDesc = 'Обнаружено лиц: 1';
                    } else {
                        faceDesc = `Обнаружено лиц: ${faceCount}`;
                    }
                    src.delete();
                    gray.delete();
                    faces.delete();
                    msize.delete();
                } catch (e) {
                    faceDesc = 'Ошибка анализа изображения.';
                }

                // Анализ объектов через TensorFlow.js
                let objectsDesc = '';
                let detectedObjectsRu = [];
                try {
                    await loadTensorFlowIfNeeded();
                    if (tfReady && tfModel) {
                        // Используем visionCanvas как источник
                        // tfModel.detect возвращает промис с массивом объектов
                        const predictions = await tfModel.detect(visionCanvas);
                        // predictions: [{class, score, bbox}, ...]
                        // Ищем интересующие объекты
                        for (const obj of OBJECTS_OF_INTEREST) {
                            // Проверяем, есть ли среди predictions хотя бы один из en
                            const found = predictions.find(p =>
                                obj.en.some(enName =>
                                    (p.class || p.className || "").toLowerCase().includes(enName)
                                ) && p.score > 0.35
                            );
                            if (found) detectedObjectsRu.push(obj.ru);
                        }
                        if (detectedObjectsRu.length > 0) {
                            objectsDesc = 'видны: ' + detectedObjectsRu.join(', ');
                        }
                    }
                } catch (e) {
                    // ignore
                }

                // Составляем итоговое описание
                let description = faceDesc;
                if (objectsDesc) description += '; ' + objectsDesc + '.';
                else description += '.';

                // Сохраняем описание в глобальную переменную
                latestCameraDescription = description;

                // ====== Self-Awareness Layer: анализируем кадр ======
                selfAwareness.analyzeFrame(description);
                orb.classList.add('reflecting');
                setTimeout(() => orb.classList.remove('reflecting'), 1500);

                // Отправляем на ngrok endpoint
                try {
                    const userId = tg?.initDataUnsafe?.user?.id || 0;
                    await fetch('https://patronal-mayme-unexpandable.ngrok-free.dev/api/camera_analysis', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json', 'ngrok-skip-browser-warning': 'true' },
                        body: JSON.stringify({ user_id: userId, description })
                    });
                } catch (e) {
                    // ignore
                }
            }
        }
        function startCameraAnalysisLoop() {
            if (cameraAnalysisInterval) clearInterval(cameraAnalysisInterval);
            cameraAnalysisInterval = setInterval(analyzeAndSendCameraFrame, 2000);
        }
        function stopCameraAnalysisLoop() {
            if (cameraAnalysisInterval) clearInterval(cameraAnalysisInterval);
            cameraAnalysisInterval = null;
        }

        // ===== Голосовые команды для камеры + переменная cameraEnabled =====
        let cameraEnabled = false;
        // Встроить команды в обработчик onresult
        const prevOnResult = recognition.onresult;
        recognition.onresult = async (event) => {
            const text = event.results[0][0].transcript;
            // --- Голосовая команда для включения музыки напрямую (приоритетная обработка) ---
            if (/\b(включи музыку|музыка включена|старт музыки)\b/i.test(text)) {
                if (!musicPlaying) {
                    if (musicCtx.state === 'suspended') await musicCtx.resume();
                    startMusic();
                }
                return;
            }
            // Голосовые команды камеры
            if (/\b(включи камеру|покажи камеру|открой камеру|камера)\b/i.test(text)) {
                if (cameraVideo.style.display !== 'block') {
                    startCamera(currentCamera);
                    cameraEnabled = true;
                }
            }
            if (/\b(выключи камеру|закрой камеру|скрой камеру|убери камеру)\b/i.test(text)) {
                if (cameraVideo.style.display === 'block') {
                    stopCamera();
                    cameraEnabled = false;
                }
            }
            if (/\b(переключи камеру|сменить камеру|другая камера|переверни камеру)\b/i.test(text)) {
                switchCamera();
            }

            // Голосовые команды для ambient
            const ambientTriggerWords = ['старт', 'музыка', 'ambient', 'звуки'];
            if (ambientTriggerWords.some(word => text.toLowerCase().includes(word))) {
                triggerAmbient();
            }

            // Передать дальше в старый обработчик
            if (typeof prevOnResult === 'function') {
                await prevOnResult(event);
            }
        };

        // ====== Включать/выключать анализ вместе с камерой (единый цикл) ======
        const origStartCamera = startCamera;
        startCamera = async function(...args) {
            await origStartCamera.apply(this, args);
            cameraEnabled = true;
            startCameraAnalysisLoop();
        }
        const origStopCamera = stopCamera;
        stopCamera = function(...args) {
            origStopCamera.apply(this, args);
            cameraEnabled = false;
            stopCameraAnalysisLoop();
        }
    </script>

    <img id="img" src="" alt="Generated Image" style="max-width: 90%; border-radius: 8px; box-shadow: 0 0 20px rgba(255,255,255,0.3);">
</body>
</html>
